<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Datasheet for Game Commentary Datasets</title>
    <link>http://localhost:1313/AIGGC-datasheet/</link>
    <description>Recent content on Datasheet for Game Commentary Datasets</description>
    <generator>Hugo -- 0.147.2</generator>
    <language>en-us</language>
    <atom:link href="http://localhost:1313/AIGGC-datasheet/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>{MOBA}-{E}2{C}: Generating {MOBA} Game Commentaries via Capturing Highlight Events from the Meta-Data</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/moba-e2c-generating-moba-game-commentaries-via-capturing-highlight-events-from-the-meta-data/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/moba-e2c-generating-moba-game-commentaries-via-capturing-highlight-events-from-the-meta-data/</guid>
      <description>&lt;p&gt;MOBA (Multiplayer Online Battle Arena) games such as Dota2 are currently one of the most popular e-sports gaming genres. Following professional commentaries is a great way to understand and enjoy a MOBA game. However, massive game competitions lack commentaries because of the shortage of professional human commentators. As an alternative, employing machine commentators that can work at any time and place is a feasible solution. Considering the challenges in modeling MOBA games, we propose a data-driven MOBA commentary generation framework, MOBA-E2C, allowing a model to generate commentaries based on the game meta-data. Subsequently, to alleviate the burden of collecting supervised data, we propose a MOBA-FuseGPT generator to generate MOBA game commentaries by fusing the power of a rule-based generator and a generative GPT generator. Finally, in the experiments, we take a popular MOBA game Dota2 as our case and construct a Chinese Dota2 commentary generation dataset Dota2-Commentary. Experimental results demonstrate the superior performance of our approach. To the best of our knowledge, this work is the first Dota2 machine commentator and Dota2-Commentary is the first dataset.&lt;/p&gt;</description>
    </item>
    <item>
      <title>{SOCCER}: An Information-Sparse Discourse State Tracking Collection in the Sports Commentary Domain</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/soccer-an-information-sparse-discourse-state-tracking-collection-in-the-sports-commentary-domain/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/soccer-an-information-sparse-discourse-state-tracking-collection-in-the-sports-commentary-domain/</guid>
      <description>&lt;p&gt;In the pursuit of natural language understanding, there has been a long standing interest in tracking state changes throughout narratives. Impressive progress has been made in modeling the state of transaction-centric dialogues and procedural texts. However, this problem has been less intensively studied in the realm of general discourse where ground truth descriptions of states may be loosely defined and state changes are less densely distributed over utterances. This paper proposes to turn to simplified, fully observable systems that show some of these properties: Sports events. We curated 2,263 soccer matches including time-stamped natural language commentary accompanied by discrete events such as a team scoring goals, switching players or being penalized with cards. We propose a new task formulation where, given paragraphs of commentary of a game at different timestamps, the system is asked to recognize the occurrence of in-game events. This domain allows for rich descriptions of state while avoiding the complexities of many other real-world settings. As an initial point of performance measurement, we include two baseline methods from the perspectives of sentence classification with temporal dependence and current state-of-the-art generative model, respectively, and demonstrate that even sophisticated existing methods struggle on the state tracking task when the definition of state broadens or non-event chatter becomes prevalent.&lt;/p&gt;</description>
    </item>
    <item>
      <title>A Descriptive Basketball Highlight Dataset for Automatic Commentary Generation</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/a-descriptive-basketball-highlight-dataset-for-automatic-commentary-generation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/a-descriptive-basketball-highlight-dataset-for-automatic-commentary-generation/</guid>
      <description>&lt;p&gt;The emergence of video captioning makes it possible to automatically generate natural language description for a given video. However, generating detailed video descriptions that incorporate domain-specific information remains an unsolved challenge, holding significant research and application value, particularly in domains such as sports commentary generation. Moreover, sports event commentary goes beyond being a mere game report, it involves entertaining, metaphorical, and emotional descriptions. To promote the field of sports commentary automatic generation, in this paper, we introduce a novel dataset, the Basketball Highlight Commentary (BH-Commentary), comprising approximately 4K basketball highlight videos with groundtruth commentaries from professional commentators. In addition, we propose an end-to-end framework as a benchmark for basketball highlight commentary generation task, in which a lightweight and effective prompt strategy is designed to enhance alignment fusion among visual and textual features. Experimental results on the BH-Commentary dataset demonstrate the validity of the dataset and the effectiveness of the proposed benchmark for sports highlight commentary generation.&lt;/p&gt;</description>
    </item>
    <item>
      <title>A Hybrid Deep Learning Model for Automated Cricket Commentary Generation</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/a-hybrid-deep-learning-model-for-automated-cricket-commentary-generation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/a-hybrid-deep-learning-model-for-automated-cricket-commentary-generation/</guid>
      <description>&lt;p&gt;The paper proposes an innovative method for generating cricket commentary. A hybrid model is proposed that combines three types of neural networks: Convolutional Neural Networks (CNN) for image processing, Long Short-Term Memory (LSTM) networks for sequential text generation, and Graph Convolutional Networks (GCN) for semantic understanding. By integrating these components, the model can generate ball-by-ball commentary that is coherent and contextaware. The model works by processing video frames from a cricket match using CNN. The resulting feature maps are used to retain essential visual information. Fully connected layers transform the features to a format suitable for input into the LSTM. The LSTM generates one word at a time, considering the temporal dependencies inherent in ball-by-ball events. To enhance the semantic understanding between the generated captions, the GCN is used. Evaluation metrics like BLEU, METEOR, and ROUGE are used to assess the proficiency of the model.&lt;/p&gt;</description>
    </item>
    <item>
      <title>A Japanese Chess Commentary Corpus</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/a-japanese-chess-commentary-corpus/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/a-japanese-chess-commentary-corpus/</guid>
      <description>&lt;p&gt;In recent years there has been a surge of interest in the natural language prosessing related to the real world, such as symbol grounding, language generation, and nonlinguistic data search by natural language queries. In order to concentrate on language ambiguities, we propose to use a well-defined {\textquotedblleft}real world,{\textquotedblright} that is game states. We built a corpus consisting of pairs of sentences and a game state. The game we focus on is shogi (Japanese chess). We collected 742,286 commentary sentences in Japanese. They are spontaneously generated contrary to natural language annotations in many image datasets provided by human workers on Amazon Mechanical Turk. We defined domain specific named entities and we segmented 2,508 sentences into words manually and annotated each word with a named entity tag. We describe a detailed definition of named entities and show some statistics of our game commentary corpus. We also show the results of the experiments of word segmentation and named entity recognition. The accuracies are as high as those on general domain texts indicating that we are ready to tackle various new problems related to the real world.&lt;/p&gt;</description>
    </item>
    <item>
      <title>About</title>
      <link>http://localhost:1313/AIGGC-datasheet/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/about/</guid>
      <description></description>
    </item>
    <item>
      <title>Annotating Event Appearance for Japanese Chess Commentary Corpus</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/annotating-event-appearance-for-japanese-chess-commentary-corpus/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/annotating-event-appearance-for-japanese-chess-commentary-corpus/</guid>
      <description>&lt;p&gt;In recent years, there has been a surge of interest in natural language processing related to the real world, such as symbol grounding, language generation, and non-linguistic data search by natural language queries. Researchers usually collect pairs of text and non-text data for research. However, the text and non-text data are not always a {\textquotedblleft}true{\textquotedblright} pair. We focused on the shogi (Japanese chess) commentaries, which are accompanied by game states as a well-defined {\textquotedblleft}real world{\textquotedblright}. For analyzing and processing texts accurately, considering only the given states is insufficient, and we must consider the relationship between texts and the real world. In this paper, we propose {\textquotedblleft}Event Appearance{\textquotedblright} labels that show the relationship between events mentioned in texts and those happening in the real world. Our event appearance label set consists of temporal relation, appearance probability, and evidence of the event. Statistics of the annotated corpus and the experimental result show that there exists temporal relation which skillful annotators realize in common. However, it is hard to predict the relationship only by considering the given states.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Annotating Modality Expressions and Event Factuality for a{Japanese Chess Commentary Corpus</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/annotating-modality-expressions-and-event-factuality-for-a-japanese-chess-commentary-corpus/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/annotating-modality-expressions-and-event-factuality-for-a-japanese-chess-commentary-corpus/</guid>
      <description>&lt;p&gt;In recent years, there has been a surge of interest in the natural language processing related to the real world, such as symbol grounding, language generation, and nonlinguistic data search by natural language queries. We argue that shogi (Japanese chess) commentaries, which are accompanied by game states, are an interesting testbed for these tasks. A commentator refers not only to the current board state but to past and future moves, and yet such references can be grounded in the game tree, possibly with the help of modern game-tree search algorithms. For this reason, we previously collected shogi commentaries together with board states and have been developing a game commentary generator. In this paper, we augment the corpus with manual annotation of modality expressions and event factuality. The annotated corpus includes 1,622 modality expressions, 5,014 event class tags and 3,092 factuality tags. It can be used to train a computer to identify words and phrases that signal factuality and to determine events with the said factuality, paving the way for grounding possible and counterfactual states.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Aspect-based Sentiment Evaluation of Chess Moves (ASSESS): an NLP-based Method for Evaluating Chess Strategies from Textbooks</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/aspect-based-sentiment-evaluation-of-chess-moves-assess-an-nlp-based-method-for-evaluating-chess-strategies-from-textbooks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/aspect-based-sentiment-evaluation-of-chess-moves-assess-an-nlp-based-method-for-evaluating-chess-strategies-from-textbooks/</guid>
      <description>&lt;p&gt;The chess domain is well-suited for creating an artificial intelligence (AI) system that mimics real-world challenges, including decision-making. Throughout the years, minimal attention has been paid to investigating insights derived from unstructured chess data sources. In this study, we examine the complicated relationships between multiple referenced moves in a chess-teaching textbook, and propose a novel method designed to encapsulate chess knowledge derived from move-action phrases. This study investigates the feasibility of using a modified sentiment analysis method as a means for evaluating chess moves based on text. Our proposed Aspect-Based Sentiment Analysis (ABSA) method represents an advancement in evaluating the sentiment associated with referenced chess moves. By extracting insights from move-action phrases, our approach aims to provide a more fine-grained and contextually aware {\textquoteleft}chess move&amp;rsquo;-based sentiment classification. Through empirical experiments and analysis, we evaluate the performance of our fine-tuned ABSA model, presenting results that confirm the efficiency of our approach in advancing aspect-based sentiment classification within the chess domain. This research contributes to the area of game-playing by machines and shows the practical applicability of leveraging NLP techniques to understand the context of strategic games. Keywords: Natural Language Processing, Chess, Aspect-based Sentiment Analysis (ABSA), Chess Move Evaluation.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Audio Commentary System for Real-Time Racing Game Play</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/audio-commentary-system-for-real-time-racing-game-play/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/audio-commentary-system-for-real-time-racing-game-play/</guid>
      <description>&lt;p&gt;Live commentaries are essential for enhancing spectators&amp;rsquo; enjoyment and understanding during sports events or e-sports streams. We introduce a live audio commentator system designed specifically for a racing game, driven by the high demand in the e-sports field. While a player is playing a racing game, our system tracks real-time user play data including speed and steer rotations, and generates commentary to accompany the live stream. Human evaluation suggested that generated commentary enhances enjoyment and understanding of races compared to streams without commentary. Incorporating additional modules to improve diversity and detect irregular events, such as course-outs and collisions, further increases the preference for the output commentaries.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Automated cricket commentary generation using deep learning</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/automated-cricket-commentary-generation-using-deep-learning/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/automated-cricket-commentary-generation-using-deep-learning/</guid>
      <description>&lt;p&gt;This work presents an automated and novel system for cricket commentary generation by the introduction of event driven approach and image captioning features. The presented system uses artificial intelligence and machine learning (ML) based methods to investigate real-time match data and generate real-time commentary by Image Captioning technique. For this purpose, deep neural network-based models and digital image processing techniques are used to detect the significant moments in the real-time match and generate commentary based on these real-time match events. The proposed method has been assessed using a dataset of 2 hours live cricket matches of India and England. After processing the match video, it has been observed that the developed model is successfully able to generate high-quality real-time commentary with a significant amount of accuracy. By commissioning leading-edge deep neural network-based model, the developed model determines the fitness to generate subtitles that are not only precise but also contextually appropriate, and efficiently apprehending the essence of the input frames.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Automated Story Selection for Color Commentary in Sports</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/automated-story-selection-for-color-commentary-in-sports/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/automated-story-selection-for-color-commentary-in-sports/</guid>
      <description>&lt;p&gt;Automated sports commentary is a form of automated narrative. Sports commentary exists to keep the viewer informed and entertained. One way to entertain the viewer is by telling brief stories relevant to the game in progress. We present a system called the sports commentary recommendation system (SCoReS) that can automatically suggest stories for commentators to tell during games. Through several user studies, we compared commentary using SCoReS to three other types of commentary and show that SCoReS adds significantly to the broadcast across several enjoyment metrics. We also collected interview data from professional sports commentators who positively evaluated a demonstration of the system. We conclude that SCoReS can be a useful broadcast tool, effective at selecting stories that add to the enjoyment and watchability of sports. SCoReS is a step toward automating sports commentary and, thus, automating narrative.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Ball-by-Ball Cricket Commentary Generation using Stateful Sequence-to-Sequence Model</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/ball-by-ball-cricket-commentary-generation-using-stateful-sequence-to-sequence-model/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/ball-by-ball-cricket-commentary-generation-using-stateful-sequence-to-sequence-model/</guid>
      <description>&lt;p&gt;Due to the availability of high performance computational devices and enormous video data, deep learning algorithms are assisting for human understandable description of videos. Automatic commentary generation of cricket videos take advantage of aforementioned intelligent techniques. VGG-16 network facilitates extraction of visual pattern from frames followed by encoder-decoder LSTM model. Proposed model can handle variable length input data to output variable number of sequential output. Moreover, the model has ability to encompass temporal information to predict the line and length bowled by bowler, the shot selection of batsman and outcome of the ball. Due to unavailability of cricket commentary dataset, a novel cricket commentary dataset containing video-commentary pairs is presented. Evaluation is also performed on benchmark video captioning datasets which are Microsoft Video Description Dataset (MSVD) and MSR - Video to Text dataset (MSRVTT). Captions generated by our model are evaluated on video captioning metrics which are METEOR, BLEU, ROGUE L and CIDEr and outperforms the baseline model.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Commentary Generation from Data Records of Multiplayer Strategy Esports Game</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/commentary-generation-from-data-records-of-multiplayer-strategy-esports-game/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/commentary-generation-from-data-records-of-multiplayer-strategy-esports-game/</guid>
      <description>&lt;p&gt;Esports, a sports competition on video games, has become one of the most important sporting events. Although esports play logs have been accumulated, only a small portion of them accompany text commentaries for the audience to retrieve and understand the plays. In this study, we therefore introduce the task of generating game commentaries from esports&amp;rsquo; data records. We first build large-scale esports data-to-text datasets that pair structured data and commentaries from a popular esports game, League of Legends. We then evaluate Transformer-based models to generate game commentaries from structured data records, while examining the impact of the pre-trained language models. Evaluation results on our dataset revealed the challenges of this novel task. We will release our dataset to boost potential research in the data-to-text generation community.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Conceptual Representation and Evaluation of an FPS Game Commentary Generator</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/conceptual-representation-and-evaluation-of-an-fps-game-commentary-generator/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/conceptual-representation-and-evaluation-of-an-fps-game-commentary-generator/</guid>
      <description>&lt;p&gt;Playing video games has been popular across all the age limits of modern society. In the beginning, it was limited among the younger community and it was just a hobby limited to individuals. Even though the majority of society sees video gaming as having a negative impact on society, this modern industry acts a significant role in healing the present competitive, stressful society. Game commentary has played a major role in the domain of competitive ESports. A proper game commentary is beneficial to both players and the audience. The aim of this project is to analyze the gameplays to produce a commentary track while balancing the contributing factors, color commentary, and play-by-play commentary. The project consists of three modules that perform the study in three perspectives: 1. Word sets related to action, spatial, temporal, and statistical information, 2. Word sets related to color commentary, 3. Word sets related to play-by-play commentary. In each module, a game commentary is generated using only the word sets related to that module. For evaluation, the similarity between the human commentary and the generated commentaries individually will be calculated.&lt;/p&gt;</description>
    </item>
    <item>
      <title>CS-lol: a Dataset of Viewer Comment with Scene in E-sports Live-streaming</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/cs-lol-a-dataset-of-viewer-comment-with-scene-in-e-sports-live-streaming/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/cs-lol-a-dataset-of-viewer-comment-with-scene-in-e-sports-live-streaming/</guid>
      <description>&lt;p&gt;Billions of live-streaming viewers share their opinions on scenes they are watching in real-time and interact with the event, commentators as well as other viewers via text comments. Thus, there is necessary to explore viewersâ€™ comments with scenes in E-sport live-streaming events. In this paper, we developed CS-lol, a new large-scale dataset containing comments from viewers paired with descriptions of game scenes in E-sports live-streaming. Moreover, we propose a task, namely viewer comment retrieval, to retrieve the viewer comments for the scene of the live-streaming event. Results on a series of baseline retrieval methods derived from typical IR evaluation methods show our task as a challenging task. Finally, we release CS-lol and baseline implementation to the research community as a resource.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Datasheet</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasheet/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasheet/</guid>
      <description>&lt;table&gt;
  &lt;thead&gt;
      &lt;tr&gt;
          &lt;th&gt;Name&lt;/th&gt;
          &lt;th&gt;Category&lt;/th&gt;
          &lt;th&gt;Game&lt;/th&gt;
          &lt;th&gt;Commentary&lt;/th&gt;
          &lt;th&gt;Game-State&lt;/th&gt;
          &lt;th&gt;Video(h)&lt;/th&gt;
          &lt;th&gt;Image&lt;/th&gt;
          &lt;th&gt;Structured Data&lt;/th&gt;
          &lt;th&gt;Other&lt;/th&gt;
      &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://aclanthology.org/P18-1154/&#34;&gt;Chess Benchmark&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Board&lt;/td&gt;
          &lt;td&gt;Chess&lt;/td&gt;
          &lt;td&gt;298,000&lt;/td&gt;
          &lt;td&gt;298,000&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;6 Categories&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://api.semanticscholar.org/CorpusID:197935466&#34;&gt;SentiMATE Dataset&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Board&lt;/td&gt;
          &lt;td&gt;Chess&lt;/td&gt;
          &lt;td&gt;15,000&lt;/td&gt;
          &lt;td&gt;15,000&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;Sentiment&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://aclanthology.org/2024.games-1.5/&#34;&gt;ABSA Dataset&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Board&lt;/td&gt;
          &lt;td&gt;Chess&lt;/td&gt;
          &lt;td&gt;622&lt;/td&gt;
          &lt;td&gt;622&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;Triple&lt;/td&gt;
          &lt;td&gt;Sentiment&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://doi.org/10.1109/CIG.2015.7317930&#34;&gt;Shogi Dataset&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Board&lt;/td&gt;
          &lt;td&gt;Shogi&lt;/td&gt;
          &lt;td&gt;218,615&lt;/td&gt;
          &lt;td&gt;218,615&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;KIF&lt;/td&gt;
          &lt;td&gt;In Japanese&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://aclanthology.org/L16-1225/&#34;&gt;Shogi Commentary Corpus&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Board&lt;/td&gt;
          &lt;td&gt;Shogi&lt;/td&gt;
          &lt;td&gt;742,286&lt;/td&gt;
          &lt;td&gt;742,286&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;In Japanese&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://aclanthology.org/L18-1393/&#34;&gt;Shogi Annotated Corpus1&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Board&lt;/td&gt;
          &lt;td&gt;Shogi&lt;/td&gt;
          &lt;td&gt;2,040&lt;/td&gt;
          &lt;td&gt;2,040&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;Labels&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://aclanthology.org/2020.lrec-1.530/&#34;&gt;Shogi Annotated Corpus2&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Board&lt;/td&gt;
          &lt;td&gt;Shogi&lt;/td&gt;
          &lt;td&gt;2,040&lt;/td&gt;
          &lt;td&gt;2,040&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;Event appearance&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://api.semanticscholar.org/CorpusID:248218465&#34;&gt;Go Annotated Dataset&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Board&lt;/td&gt;
          &lt;td&gt;Go&lt;/td&gt;
          &lt;td&gt;458,182&lt;/td&gt;
          &lt;td&gt;458,182&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://doi.org/10.1109/CIG.2016.7860441&#34;&gt;Move Quality Dataset&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Board&lt;/td&gt;
          &lt;td&gt;Go&lt;/td&gt;
          &lt;td&gt;4,836&lt;/td&gt;
          &lt;td&gt;4,836&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;837 Bad Moves&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://api.semanticscholar.org/CorpusID:270737995&#34;&gt;Guandan Dataset&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Board&lt;/td&gt;
          &lt;td&gt;Guandan&lt;/td&gt;
          &lt;td&gt;Yes&lt;/td&gt;
          &lt;td&gt;Yes&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;http://dx.doi.org/10.1109/CVPRW.2018.00223&#34;&gt;SoccerNet&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Sports&lt;/td&gt;
          &lt;td&gt;Soccer&lt;/td&gt;
          &lt;td&gt;506,173&lt;/td&gt;
          &lt;td&gt;6,637&lt;/td&gt;
          &lt;td&gt;764&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;3 Categories&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://doi.org/10.1109/CVPRW53098.2021.00508&#34;&gt;SoccerNet-v2&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Sports&lt;/td&gt;
          &lt;td&gt;Soccer&lt;/td&gt;
          &lt;td&gt;506,173&lt;/td&gt;
          &lt;td&gt;6,637&lt;/td&gt;
          &lt;td&gt;764&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;17 Categories&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://api.semanticscholar.org/CorpusID:249894209&#34;&gt;SoccerNet-v3&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Sports&lt;/td&gt;
          &lt;td&gt;Soccer&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;1,324,732&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;33,986&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://api.semanticscholar.org/CorpusID:269757092&#34;&gt;SoccerNet-Echoes&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Sports&lt;/td&gt;
          &lt;td&gt;Soccer&lt;/td&gt;
          &lt;td&gt;Yes&lt;/td&gt;
          &lt;td&gt;Yes&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;Audio and Multi-language&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://api.semanticscholar.org/CorpusID:258049025&#34;&gt;SoccerNet-Caption&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Sports&lt;/td&gt;
          &lt;td&gt;Soccer&lt;/td&gt;
          &lt;td&gt;36,894&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;715.9&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;Manual Aligned&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://api.semanticscholar.org/CorpusID:257766604&#34;&gt;GOAL&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Sports&lt;/td&gt;
          &lt;td&gt;Soccer&lt;/td&gt;
          &lt;td&gt;22,000+&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;25+&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;42,000+ knowledge triples&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://aclanthology.org/2024.emnlp-main.99/&#34;&gt;MatchTime&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Sports&lt;/td&gt;
          &lt;td&gt;Soccer&lt;/td&gt;
          &lt;td&gt;32,743&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;715.9&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;Automated Aligned&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://aclanthology.org/2021.naacl-main.342/&#34;&gt;SOCCER&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Sports&lt;/td&gt;
          &lt;td&gt;Soccer&lt;/td&gt;
          &lt;td&gt;32,743&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;715.9&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;Automated Aligned&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://api.semanticscholar.org/CorpusID:57854642&#34;&gt;OptaSports&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Sports&lt;/td&gt;
          &lt;td&gt;Soccer&lt;/td&gt;
          &lt;td&gt;450,000+&lt;/td&gt;
          &lt;td&gt;17,140&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://api.semanticscholar.org/CorpusID:268587932&#34;&gt;Commentary-Frame&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Sports&lt;/td&gt;
          &lt;td&gt;Cricket&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;Yes&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;Frames&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;Two Camera Angles&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://doi.org/10.1109/CCEM48484.2019.00012&#34;&gt;Ball-by-Ball Classification&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Sports&lt;/td&gt;
          &lt;td&gt;Cricket&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;1,559&lt;/td&gt;
          &lt;td&gt;1+&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://doi.org/10.1109/INMIC50486.2020.9318089&#34;&gt;Cricinfo Dataset&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Sports&lt;/td&gt;
          &lt;td&gt;Cricket&lt;/td&gt;
          &lt;td&gt;213,000&lt;/td&gt;
          &lt;td&gt;17,140&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://doi.org/10.1109/ICCCMLA63077.2024.10871604&#34;&gt;Flickr+Cricket&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Sports&lt;/td&gt;
          &lt;td&gt;Cricket&lt;/td&gt;
          &lt;td&gt;43,925&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;8,785&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;Mixed Dataset&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://api.semanticscholar.org/CorpusID:212473004&#34;&gt;Cricket Story Set&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Sports&lt;/td&gt;
          &lt;td&gt;Cricket&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;Stories&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://doi.org/10.1109/ComTech52583.2021.9616676&#34;&gt;Cricket Dataset&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Sports&lt;/td&gt;
          &lt;td&gt;Cricket&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;0.2&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://doi.org/10.1109/TMM.2016.2614221&#34;&gt;GameFlow Dataset&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Sports&lt;/td&gt;
          &lt;td&gt;Basketball&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;Yes&lt;/td&gt;
          &lt;td&gt;Yes&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;Table&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://doi.org/10.1109/TPAMI.2019.2946823&#34;&gt;FSN&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Sports&lt;/td&gt;
          &lt;td&gt;Basketball&lt;/td&gt;
          &lt;td&gt;6,520&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;3+&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://doi.org/10.1109/TPAMI.2019.2946823&#34;&gt;SVN&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Sports&lt;/td&gt;
          &lt;td&gt;Basketball&lt;/td&gt;
          &lt;td&gt;9,632&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;7.7&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://doi.org/10.1145/3664647.3681178&#34;&gt;BH-Commentary&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Sports&lt;/td&gt;
          &lt;td&gt;Basketball&lt;/td&gt;
          &lt;td&gt;4.3k&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;10.1&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;Game Highlight&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;Baseball Story Set1&lt;/td&gt;
          &lt;td&gt;Sports&lt;/td&gt;
          &lt;td&gt;Baseball&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;110 Stories&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://doi.org/10.1109/TCIAIG.2013.2275199&#34;&gt;Baseball Story Set2&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Sports&lt;/td&gt;
          &lt;td&gt;Baseball&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;Statistics in XML&lt;/td&gt;
          &lt;td&gt;110 Stories&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://doi.org/10.1109/ICPR.2016.7900036&#34;&gt;Tennis Dataset&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Sports&lt;/td&gt;
          &lt;td&gt;Tennis&lt;/td&gt;
          &lt;td&gt;633&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;1.5&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://doi.org/10.1109/DICTA.2017.8227494&#34;&gt;TenniSet&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Sports&lt;/td&gt;
          &lt;td&gt;Tennis&lt;/td&gt;
          &lt;td&gt;746&lt;/td&gt;
          &lt;td&gt;4,000+&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;6 Categories&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://doi.org/10.1109/CVPRW53098.2021.00513&#34;&gt;LoL-V2T&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Esports&lt;/td&gt;
          &lt;td&gt;LoL&lt;/td&gt;
          &lt;td&gt;62,677&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;76+&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;Multiple Captions Per Clip&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://api.semanticscholar.org/CorpusID:235327865&#34;&gt;LoL19&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Esports&lt;/td&gt;
          &lt;td&gt;LoL&lt;/td&gt;
          &lt;td&gt;3,490&lt;/td&gt;
          &lt;td&gt;3,490&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;JSON&lt;/td&gt;
          &lt;td&gt;10 Categories&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://aclanthology.org/2024.naacl-srw.28/&#34;&gt;LoL19-21&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Esports&lt;/td&gt;
          &lt;td&gt;LoL&lt;/td&gt;
          &lt;td&gt;10,590&lt;/td&gt;
          &lt;td&gt;10,590&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;JSON&lt;/td&gt;
          &lt;td&gt;10 Categories&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://api.semanticscholar.org/CorpusID:269456978&#34;&gt;Game-MUg&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Esports&lt;/td&gt;
          &lt;td&gt;LoL&lt;/td&gt;
          &lt;td&gt;70,711&lt;/td&gt;
          &lt;td&gt;15,221&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;Audio Features&lt;/td&gt;
          &lt;td&gt;3,657,611 Audience Chat&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://doi.org/10.1145/3576840.3578334&#34;&gt;CS-lol&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Esports&lt;/td&gt;
          &lt;td&gt;LoL&lt;/td&gt;
          &lt;td&gt;10,590&lt;/td&gt;
          &lt;td&gt;10,590&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;All Audience Chat&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://doi.org/10.1109/CIG.2019.8848060&#34;&gt;Multimodal LoL Dataset&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Esports&lt;/td&gt;
          &lt;td&gt;LoL&lt;/td&gt;
          &lt;td&gt;23,411&lt;/td&gt;
          &lt;td&gt;24,770&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;Annotated With Affect and Context&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://aclanthology.org/2022.findings-emnlp.333/&#34;&gt;Dota2-Commentary&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Esports&lt;/td&gt;
          &lt;td&gt;Dota2&lt;/td&gt;
          &lt;td&gt;7,473&lt;/td&gt;
          &lt;td&gt;7,473&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;175,627 Generated Comments&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://api.semanticscholar.org/CorpusID:2488088&#34;&gt;Robocup Sportscasts1&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Esports&lt;/td&gt;
          &lt;td&gt;RoboCup&lt;/td&gt;
          &lt;td&gt;2,036&lt;/td&gt;
          &lt;td&gt;10,452&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;Sensor Data&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://api.semanticscholar.org/CorpusID:6678765&#34;&gt;Robocup Sportscasts2&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Esports&lt;/td&gt;
          &lt;td&gt;RoboCup&lt;/td&gt;
          &lt;td&gt;21,314&lt;/td&gt;
          &lt;td&gt;4,314&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;Sensor Data&lt;/td&gt;
          &lt;td&gt;English and Korea&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://aclanthology.org/2021.inlg-1.11/&#34;&gt;Racing Commentary Dataset1&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Esports&lt;/td&gt;
          &lt;td&gt;Assetto Corsa&lt;/td&gt;
          &lt;td&gt;129,226&lt;/td&gt;
          &lt;td&gt;226&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;List&lt;/td&gt;
          &lt;td&gt;Japanese&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://aclanthology.org/2023.inlg-demos.4/&#34;&gt;Racing Commentary Dataset2&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Esports&lt;/td&gt;
          &lt;td&gt;Assetto Corsa&lt;/td&gt;
          &lt;td&gt;129,226&lt;/td&gt;
          &lt;td&gt;226&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;List&lt;/td&gt;
          &lt;td&gt;English&lt;/td&gt;
      &lt;/tr&gt;
      &lt;tr&gt;
          &lt;td&gt;&lt;a href=&#34;https://api.semanticscholar.org/CorpusID:249929159&#34;&gt;FPS Commentary Dataset&lt;/a&gt;&lt;/td&gt;
          &lt;td&gt;Esports&lt;/td&gt;
          &lt;td&gt;Valorant&lt;/td&gt;
          &lt;td&gt;8,600&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
          &lt;td&gt;8,600&lt;/td&gt;
          &lt;td&gt;Annotated with Game Info&lt;/td&gt;
          &lt;td&gt;N/A&lt;/td&gt;
      &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;</description>
    </item>
    <item>
      <title>Detection and labeling of bad moves for coaching go</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/detection-and-labeling-of-bad-moves-for-coaching-go/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/detection-and-labeling-of-bad-moves-for-coaching-go/</guid>
      <description>&lt;p&gt;The level of computer programs has now reached professional strength for many games, even for the game of Go recently. A more difficult task for computer intelligence now is to create a program able to coach human players, so that they can improve their play. In this paper, we propose a method to detect and label the bad moves of human players for the game of Go. This task is challenging because even strong human players only agree at a rate of around 50% about which moves should be considered as bad. We use supervised learning with features largely available in many Go programs, and we obtain an identification level close to the one observed between strong human players. Also, an evaluation by a professional player shows that our method is already useful for intermediate-level players.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Enhancing Commentary Strategies for Imperfect Information Card Games: A Study of Large Language Models in Guandan Commentary</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/enhancing-commentary-strategies-for-imperfect-information-card-games-a-study-of-large-language-models-in-guandan-commentary/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/enhancing-commentary-strategies-for-imperfect-information-card-games-a-study-of-large-language-models-in-guandan-commentary/</guid>
      <description>&lt;p&gt;Recent advancements in large language models (LLMs) have unlocked the potential for generating high-quality game commentary. However, producing insightful and engaging commentary for complex games with incomplete information remains a significant challenge. In this paper, we introduce a novel commentary method that combine Reinforcement Learning (RL) and LLMs, tailored specifically for the Chinese card game \textit{Guandan}. Our system leverages RL to generate intricate card-playing scenarios and employs LLMs to generate corresponding commentary text, effectively emulating the strategic analysis and narrative prowess of professional commentators. The framework comprises a state commentary guide, a Theory of Mind (ToM)-based strategy analyzer, and a style retrieval module, which seamlessly collaborate to deliver detailed and context-relevant game commentary in the Chinese language environment. We empower LLMs with ToM capabilities and refine both retrieval and information filtering mechanisms. This facilitates the generation of personalized commentary content. Our experimental results showcase the substantial enhancement in performance achieved by the proposed commentary framework when applied to open-source LLMs, surpassing the performance of GPT-4 across multiple evaluation metrics.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Extraction of Strong and Weak Regions of Cricket Batsman through Text-Commentary Analysis</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/extraction-of-strong-and-weak-regions-of-cricket-batsman-through-text-commentary-analysis/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/extraction-of-strong-and-weak-regions-of-cricket-batsman-through-text-commentary-analysis/</guid>
      <description>&lt;p&gt;Cricket is a famous game in the world where many metrics are introduced and being used to help the coaches and umpires to solve the critical problems. Though different statistics are used to quantify the player&amp;rsquo;s performance based on strike rate, average or for ranking, prediction, and optimal team selection. There is not any effective method to measure the strong and weak regions of a batsman in cricket. In this paper, a text mining method is presented to extract either the strong shot selection points that are frequent for scoring or weak shot regions that seem tough for a batsman to play. Also, a mechanism is put forward to calculate the region-wise strike rate of an individual batsman. To achieve the objectives, the T20 cricket text commentary is being used for this purpose which is available on the espncricinfo website. The proposed method can be helpful for coaches and players to know the strong or weak regions where the batsman feels ease or difficulty to play, respectively. Moreover, the opponent bowlers can also use this method to plan the area where to bowl to each batsman.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Fine-Grained Video Captioning via Graph-based Multi-Granularity Interaction Learning</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/fine-grained-video-captioning-via-graph-based-multi-granularity-interaction-learning/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/fine-grained-video-captioning-via-graph-based-multi-granularity-interaction-learning/</guid>
      <description>&lt;p&gt;Learning to generate continuous linguistic descriptions for multi-subject interactive videos in great details has particular applications in team sports auto-narrative. In contrast to traditional video caption, this task is more challenging as it requires simultaneous modeling of fine-grained individual actions, uncovering of spatio-temporal dependency structures of frequent group interactions, and then accurate mapping of these complex interaction details into long and detailed commentary. To explicitly address these challenges, we propose a novel framework &lt;italic&gt;Graph-based Learning for Multi-Granularity Interaction Representation (GLMGIR)&lt;/italic&gt; for fine-grained team sports auto-narrative task. A multi-granular interaction modeling module is proposed to extract among-subjectsâ€™ interactive actions in a progressive way for encoding both intra- and inter-team interactions. Based on the above multi-granular representations, a multi-granular attention module is developed to consider action/event descriptions of multiple spatio-temporal resolutions. Both modules are integrated seamlessly and work in a collaborative way to generate the final narrative. In the meantime, to facilitate reproducible research, we collect a new video dataset from &lt;italic&gt;YouTube.com&lt;/italic&gt; called Sports Video Narrative dataset (SVN). It is a novel direction as it contains &lt;inline-formula&gt;&lt;tex-math notation=&#34;LaTeX&#34;&gt;$6K$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;a href=&#34;mml:math&#34;&gt;mml:math&lt;/a&gt;&lt;a href=&#34;mml:mrow&#34;&gt;mml:mrow&lt;/a&gt;&lt;a href=&#34;mml:mn&#34;&gt;mml:mn&lt;/a&gt;6&amp;lt;/mml:mn&amp;gt;&lt;a href=&#34;mml:mi&#34;&gt;mml:mi&lt;/a&gt;K&amp;lt;/mml:mi&amp;gt;&amp;lt;/mml:mrow&amp;gt;&amp;lt;/mml:math&amp;gt;&lt;inline-graphic xlink:href=&#34;zhuang-ieq1-2946823.gif&#34;/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; team sports videos (i.e., NBA basketball games) with &lt;inline-formula&gt;&lt;tex-math notation=&#34;LaTeX&#34;&gt;$10K$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;a href=&#34;mml:math&#34;&gt;mml:math&lt;/a&gt;&lt;a href=&#34;mml:mrow&#34;&gt;mml:mrow&lt;/a&gt;&lt;a href=&#34;mml:mn&#34;&gt;mml:mn&lt;/a&gt;10&amp;lt;/mml:mn&amp;gt;&lt;a href=&#34;mml:mi&#34;&gt;mml:mi&lt;/a&gt;K&amp;lt;/mml:mi&amp;gt;&amp;lt;/mml:mrow&amp;gt;&amp;lt;/mml:math&amp;gt;&lt;inline-graphic xlink:href=&#34;zhuang-ieq2-2946823.gif&#34;/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; ground-truth narratives(e.g., sentences). Furthermore, as previous metrics such as METEOR (i.e., used in coarse-grained video caption task) DO NOT cope with fine-grained sports narrative task well, we hence develop a novel evaluation metric named Fine-grained Captioning Evaluation (FCE), which measures how accurate the generated linguistic description reflects fine-grained action details as well as the overall spatio-temporal interactional structure. Extensive experiments on our SVN dataset have demonstrated the effectiveness of the proposed framework for fine-grained team sports video auto-narrative.&lt;/p&gt;</description>
    </item>
    <item>
      <title>From eSports Data to Game Commentary: Datasets, Models, and Evaluation Metrics</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/from-esports-data-to-game-commentary-datasets-models-and-evaluation-metrics/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/from-esports-data-to-game-commentary-datasets-models-and-evaluation-metrics/</guid>
      <description>&lt;p&gt;Abstract not available.&lt;/p&gt;
&lt;div style=&#34;margin-top: 1rem; padding: 1rem; display: inline-block;&#34;&gt;
  &lt;a href=&#34;https://api.semanticscholar.org/CorpusID:235327865&#34; target=&#34;_blank&#34; style=&#34;background-color: #0d9bdc; color: white; padding: 10px 16px; margin-right: 8px; text-decoration: none; border-radius: 4px; font-weight: bold;&#34;&gt;
    Download Paper
  &lt;/a&gt;
  &lt;a href=&#34;bib/from-esports-data-to-game-commentary-datasets-models-and-evaluation-metrics.bib&#34; download style=&#34;background-color: #f0a500; color: white; padding: 10px 16px; text-decoration: none; border-radius: 4px; font-weight: bold;&#34;&gt;
    Download BibTeX
  &lt;/a&gt;
&lt;/div&gt;</description>
    </item>
    <item>
      <title>Game-MUG: Multimodal Oriented Game Situation Understanding and Commentary Generation Dataset</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/game-mug-multimodal-oriented-game-situation-understanding-and-commentary-generation-dataset/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/game-mug-multimodal-oriented-game-situation-understanding-and-commentary-generation-dataset/</guid>
      <description>&lt;p&gt;Abstract not available.&lt;/p&gt;
&lt;div style=&#34;margin-top: 1rem; padding: 1rem; display: inline-block;&#34;&gt;
  &lt;a href=&#34;https://api.semanticscholar.org/CorpusID:269456978&#34; target=&#34;_blank&#34; style=&#34;background-color: #0d9bdc; color: white; padding: 10px 16px; margin-right: 8px; text-decoration: none; border-radius: 4px; font-weight: bold;&#34;&gt;
    Download Paper
  &lt;/a&gt;
  &lt;a href=&#34;bib/game-mug-multimodal-oriented-game-situation-understanding-and-commentary-generation-dataset.bib&#34; download style=&#34;background-color: #f0a500; color: white; padding: 10px 16px; text-decoration: none; border-radius: 4px; font-weight: bold;&#34;&gt;
    Download BibTeX
  &lt;/a&gt;
&lt;/div&gt;</description>
    </item>
    <item>
      <title>GameFlow: Narrative Visualization of NBA Basketball Games</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/gameflow-narrative-visualization-of-nba-basketball-games/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/gameflow-narrative-visualization-of-nba-basketball-games/</guid>
      <description>&lt;p&gt;Abstract not available.&lt;/p&gt;
&lt;div style=&#34;margin-top: 1rem; padding: 1rem; display: inline-block;&#34;&gt;
  &lt;a href=&#34;https://doi.org/10.1109/TMM.2016.2614221&#34; target=&#34;_blank&#34; style=&#34;background-color: #0d9bdc; color: white; padding: 10px 16px; margin-right: 8px; text-decoration: none; border-radius: 4px; font-weight: bold;&#34;&gt;
    Download Paper
  &lt;/a&gt;
  &lt;a href=&#34;bib/gameflow-narrative-visualization-of-nba-basketball-games.bib&#34; download style=&#34;background-color: #f0a500; color: white; padding: 10px 16px; text-decoration: none; border-radius: 4px; font-weight: bold;&#34;&gt;
    Download BibTeX
  &lt;/a&gt;
&lt;/div&gt;</description>
    </item>
    <item>
      <title>Generating commentaries for tennis videos</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/generating-commentaries-for-tennis-videos/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/generating-commentaries-for-tennis-videos/</guid>
      <description>&lt;p&gt;Abstract not available.&lt;/p&gt;
&lt;div style=&#34;margin-top: 1rem; padding: 1rem; display: inline-block;&#34;&gt;
  &lt;a href=&#34;https://doi.org/10.1109/ICPR.2016.7900036&#34; target=&#34;_blank&#34; style=&#34;background-color: #0d9bdc; color: white; padding: 10px 16px; margin-right: 8px; text-decoration: none; border-radius: 4px; font-weight: bold;&#34;&gt;
    Download Paper
  &lt;/a&gt;
  &lt;a href=&#34;bib/generating-commentaries-for-tennis-videos.bib&#34; download style=&#34;background-color: #f0a500; color: white; padding: 10px 16px; text-decoration: none; border-radius: 4px; font-weight: bold;&#34;&gt;
    Download BibTeX
  &lt;/a&gt;
&lt;/div&gt;</description>
    </item>
    <item>
      <title>Generating Live Soccer-Match Commentary from Play Data</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/generating-live-soccer-match-commentary-from-play-data/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/generating-live-soccer-match-commentary-from-play-data/</guid>
      <description>&lt;p&gt;Abstract not available.&lt;/p&gt;
&lt;div style=&#34;margin-top: 1rem; padding: 1rem; display: inline-block;&#34;&gt;
  &lt;a href=&#34;https://api.semanticscholar.org/CorpusID:57854642&#34; target=&#34;_blank&#34; style=&#34;background-color: #0d9bdc; color: white; padding: 10px 16px; margin-right: 8px; text-decoration: none; border-radius: 4px; font-weight: bold;&#34;&gt;
    Download Paper
  &lt;/a&gt;
  &lt;a href=&#34;bib/generating-live-soccer-match-commentary-from-play-data.bib&#34; download style=&#34;background-color: #f0a500; color: white; padding: 10px 16px; text-decoration: none; border-radius: 4px; font-weight: bold;&#34;&gt;
    Download BibTeX
  &lt;/a&gt;
&lt;/div&gt;</description>
    </item>
    <item>
      <title>Generating Racing Game Commentary from Vision, Language, and Structured Data</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/generating-racing-game-commentary-from-vision-language-and-structured-data/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/generating-racing-game-commentary-from-vision-language-and-structured-data/</guid>
      <description>&lt;p&gt;We propose the task of automatically generating commentaries for races in a motor racing game, from vision, structured numerical, and textual data. Commentaries provide information to support spectators in understanding events in races. Commentary generation models need to interpret the race situation and generate the correct content at the right moment. We divide the task into two subtasks: utterance timing identification and utterance generation. Because existing datasets do not have such alignments of data in multiple modalities, this setting has not been explored in depth. In this study, we introduce a new large-scale dataset that contains aligned video data, structured numerical data, and transcribed commentaries that consist of 129,226 utterances in 1,389 races in a game. Our analysis reveals that the characteristics of commentaries change over time or from viewpoints. Our experiments on the subtasks show that it is still challenging for a state-of-the-art vision encoder to capture useful information from videos to generate accurate commentaries. We make the dataset and baseline implementation publicly available for further research.&lt;/p&gt;</description>
    </item>
    <item>
      <title>GOAL: A Challenging Knowledge-grounded Video Captioning Benchmark for Real-time Soccer Commentary Generation</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/goal-a-challenging-knowledge-grounded-video-captioning-benchmark-for-real-time-soccer-commentary-generation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/goal-a-challenging-knowledge-grounded-video-captioning-benchmark-for-real-time-soccer-commentary-generation/</guid>
      <description>&lt;p&gt;Abstract not available.&lt;/p&gt;
&lt;div style=&#34;margin-top: 1rem; padding: 1rem; display: inline-block;&#34;&gt;
  &lt;a href=&#34;https://api.semanticscholar.org/CorpusID:257766604&#34; target=&#34;_blank&#34; style=&#34;background-color: #0d9bdc; color: white; padding: 10px 16px; margin-right: 8px; text-decoration: none; border-radius: 4px; font-weight: bold;&#34;&gt;
    Download Paper
  &lt;/a&gt;
  &lt;a href=&#34;bib/goal-a-challenging-knowledge-grounded-video-captioning-benchmark-for-real-time-soccer-commentary-generation.bib&#34; download style=&#34;background-color: #f0a500; color: white; padding: 10px 16px; text-decoration: none; border-radius: 4px; font-weight: bold;&#34;&gt;
    Download BibTeX
  &lt;/a&gt;
&lt;/div&gt;</description>
    </item>
    <item>
      <title>Learning a game commentary generator with grounded move expressions</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/learning-a-game-commentary-generator-with-grounded-move-expressions/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/learning-a-game-commentary-generator-with-grounded-move-expressions/</guid>
      <description>&lt;p&gt;Abstract not available.&lt;/p&gt;
&lt;div style=&#34;margin-top: 1rem; padding: 1rem; display: inline-block;&#34;&gt;
  &lt;a href=&#34;https://doi.org/10.1109/CIG.2015.7317930&#34; target=&#34;_blank&#34; style=&#34;background-color: #0d9bdc; color: white; padding: 10px 16px; margin-right: 8px; text-decoration: none; border-radius: 4px; font-weight: bold;&#34;&gt;
    Download Paper
  &lt;/a&gt;
  &lt;a href=&#34;bib/learning-a-game-commentary-generator-with-grounded-move-expressions.bib&#34; download style=&#34;background-color: #f0a500; color: white; padding: 10px 16px; text-decoration: none; border-radius: 4px; font-weight: bold;&#34;&gt;
    Download BibTeX
  &lt;/a&gt;
&lt;/div&gt;</description>
    </item>
    <item>
      <title>Learning to Generate Move-by-Move Commentary for Chess Games from Large-Scale Social Forum Data</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/learning-to-generate-move-by-move-commentary-for-chess-games-from-large-scale-social-forum-data/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/learning-to-generate-move-by-move-commentary-for-chess-games-from-large-scale-social-forum-data/</guid>
      <description>&lt;p&gt;Abstract not available.&lt;/p&gt;
&lt;div style=&#34;margin-top: 1rem; padding: 1rem; display: inline-block;&#34;&gt;
  &lt;a href=&#34;https://doi.org/10.18653/v1/P18-1154&#34; target=&#34;_blank&#34; style=&#34;background-color: #0d9bdc; color: white; padding: 10px 16px; margin-right: 8px; text-decoration: none; border-radius: 4px; font-weight: bold;&#34;&gt;
    Download Paper
  &lt;/a&gt;
  &lt;a href=&#34;bib/learning-to-generate-move-by-move-commentary-for-chess-games-from-large-scale-social-forum-data.bib&#34; download style=&#34;background-color: #f0a500; color: white; padding: 10px 16px; text-decoration: none; border-radius: 4px; font-weight: bold;&#34;&gt;
    Download BibTeX
  &lt;/a&gt;
&lt;/div&gt;</description>
    </item>
    <item>
      <title>Learning to sportscast: a test of grounded language acquisition</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/learning-to-sportscast-a-test-of-grounded-language-acquisition/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/learning-to-sportscast-a-test-of-grounded-language-acquisition/</guid>
      <description>&lt;p&gt;Abstract not available.&lt;/p&gt;
&lt;div style=&#34;margin-top: 1rem; padding: 1rem; display: inline-block;&#34;&gt;
  &lt;a href=&#34;https://api.semanticscholar.org/CorpusID:2488088&#34; target=&#34;_blank&#34; style=&#34;background-color: #0d9bdc; color: white; padding: 10px 16px; margin-right: 8px; text-decoration: none; border-radius: 4px; font-weight: bold;&#34;&gt;
    Download Paper
  &lt;/a&gt;
  &lt;a href=&#34;bib/learning-to-sportscast-a-test-of-grounded-language-acquisition.bib&#34; download style=&#34;background-color: #f0a500; color: white; padding: 10px 16px; text-decoration: none; border-radius: 4px; font-weight: bold;&#34;&gt;
    Download BibTeX
  &lt;/a&gt;
&lt;/div&gt;</description>
    </item>
    <item>
      <title>LoL-V2T: Large-Scale Esports Video Description Dataset</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/lol-v2t-large-scale-esports-video-description-dataset/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/lol-v2t-large-scale-esports-video-description-dataset/</guid>
      <description>&lt;p&gt;Abstract not available.&lt;/p&gt;
&lt;div style=&#34;margin-top: 1rem; padding: 1rem; display: inline-block;&#34;&gt;
  &lt;a href=&#34;https://doi.org/10.1109/CVPRW53098.2021.00513&#34; target=&#34;_blank&#34; style=&#34;background-color: #0d9bdc; color: white; padding: 10px 16px; margin-right: 8px; text-decoration: none; border-radius: 4px; font-weight: bold;&#34;&gt;
    Download Paper
  &lt;/a&gt;
  &lt;a href=&#34;bib/lol-v2t-large-scale-esports-video-description-dataset.bib&#34; download style=&#34;background-color: #f0a500; color: white; padding: 10px 16px; text-decoration: none; border-radius: 4px; font-weight: bold;&#34;&gt;
    Download BibTeX
  &lt;/a&gt;
&lt;/div&gt;</description>
    </item>
    <item>
      <title>MatchTime: Towards Automatic Soccer Game Commentary Generation</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/matchtime-towards-automatic-soccer-game-commentary-generation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/matchtime-towards-automatic-soccer-game-commentary-generation/</guid>
      <description>&lt;p&gt;Soccer is a globally popular sport with a vast audience, in this paper, we consider constructing an automatic soccer game commentary model to improve the audiences&amp;rsquo; viewing experience. In general, we make the following contributions: &lt;em&gt;First&lt;/em&gt;, observing the prevalent video-text misalignment in existing datasets, we manually annotate timestamps for 49 matches, establishing a more robust benchmark for soccer game commentary generation, termed as &lt;em&gt;SN-Caption-test-align&lt;/em&gt;; &lt;em&gt;Second&lt;/em&gt;, we propose a multi-modal temporal alignment pipeline to automatically correct and filter the existing dataset at scale, creating a higher-quality soccer game commentary dataset for training, denoted as &lt;em&gt;MatchTime&lt;/em&gt;; &lt;em&gt;Third&lt;/em&gt;, based on our curated dataset, we train an automatic commentary generation model, named &lt;strong&gt;MatchVoice&lt;/strong&gt;. Extensive experiments and ablation studies have demonstrated the effectiveness of our alignment pipeline, and training model on the curated datasets achieves state-of-the-art performance for commentary generation, showcasing that better alignment can lead to significant performance improvements in downstream tasks.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Multimodal Joint Emotion and Game Context Recognition in League of Legends Livestreams</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/multimodal-joint-emotion-and-game-context-recognition-in-league-of-legends-livestreams/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/multimodal-joint-emotion-and-game-context-recognition-in-league-of-legends-livestreams/</guid>
      <description>&lt;p&gt;Abstract not available.&lt;/p&gt;
&lt;div style=&#34;margin-top: 1rem; padding: 1rem; display: inline-block;&#34;&gt;
  &lt;a href=&#34;https://doi.org/10.1109/CIG.2019.8848060&#34; target=&#34;_blank&#34; style=&#34;background-color: #0d9bdc; color: white; padding: 10px 16px; margin-right: 8px; text-decoration: none; border-radius: 4px; font-weight: bold;&#34;&gt;
    Download Paper
  &lt;/a&gt;
  &lt;a href=&#34;bib/multimodal-joint-emotion-and-game-context-recognition-in-league-of-legends-livestreams.bib&#34; download style=&#34;background-color: #f0a500; color: white; padding: 10px 16px; text-decoration: none; border-radius: 4px; font-weight: bold;&#34;&gt;
    Download BibTeX
  &lt;/a&gt;
&lt;/div&gt;</description>
    </item>
    <item>
      <title>Outcome Classification in Cricket Using Deep Learning</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/outcome-classification-in-cricket-using-deep-learning/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/outcome-classification-in-cricket-using-deep-learning/</guid>
      <description>&lt;p&gt;Abstract not available.&lt;/p&gt;
&lt;div style=&#34;margin-top: 1rem; padding: 1rem; display: inline-block;&#34;&gt;
  &lt;a href=&#34;https://doi.org/10.1109/CCEM48484.2019.00012&#34; target=&#34;_blank&#34; style=&#34;background-color: #0d9bdc; color: white; padding: 10px 16px; margin-right: 8px; text-decoration: none; border-radius: 4px; font-weight: bold;&#34;&gt;
    Download Paper
  &lt;/a&gt;
  &lt;a href=&#34;bib/outcome-classification-in-cricket-using-deep-learning.bib&#34; download style=&#34;background-color: #f0a500; color: white; padding: 10px 16px; text-decoration: none; border-radius: 4px; font-weight: bold;&#34;&gt;
    Download BibTeX
  &lt;/a&gt;
&lt;/div&gt;</description>
    </item>
    <item>
      <title>Scaling up SoccerNet with Multi-view Spatial Localization and Re-identification</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/scaling-up-soccernet-with-multi-view-spatial-localization-and-re-identification/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/scaling-up-soccernet-with-multi-view-spatial-localization-and-re-identification/</guid>
      <description>&lt;p&gt;Abstract not available.&lt;/p&gt;
&lt;div style=&#34;margin-top: 1rem; padding: 1rem; display: inline-block;&#34;&gt;
  &lt;a href=&#34;https://api.semanticscholar.org/CorpusID:249894209&#34; target=&#34;_blank&#34; style=&#34;background-color: #0d9bdc; color: white; padding: 10px 16px; margin-right: 8px; text-decoration: none; border-radius: 4px; font-weight: bold;&#34;&gt;
    Download Paper
  &lt;/a&gt;
  &lt;a href=&#34;bib/scaling-up-soccernet-with-multi-view-spatial-localization-and-re-identification.bib&#34; download style=&#34;background-color: #f0a500; color: white; padding: 10px 16px; text-decoration: none; border-radius: 4px; font-weight: bold;&#34;&gt;
    Download BibTeX
  &lt;/a&gt;
&lt;/div&gt;</description>
    </item>
    <item>
      <title>SentiMATE: Learning to play Chess through Natural Language Processing</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/sentimate-learning-to-play-chess-through-natural-language-processing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/sentimate-learning-to-play-chess-through-natural-language-processing/</guid>
      <description>&lt;p&gt;Abstract not available.&lt;/p&gt;
&lt;div style=&#34;margin-top: 1rem; padding: 1rem; display: inline-block;&#34;&gt;
  &lt;a href=&#34;https://api.semanticscholar.org/CorpusID:197935466&#34; target=&#34;_blank&#34; style=&#34;background-color: #0d9bdc; color: white; padding: 10px 16px; margin-right: 8px; text-decoration: none; border-radius: 4px; font-weight: bold;&#34;&gt;
    Download Paper
  &lt;/a&gt;
  &lt;a href=&#34;bib/sentimate-learning-to-play-chess-through-natural-language-processing.bib&#34; download style=&#34;background-color: #f0a500; color: white; padding: 10px 16px; text-decoration: none; border-radius: 4px; font-weight: bold;&#34;&gt;
    Download BibTeX
  &lt;/a&gt;
&lt;/div&gt;</description>
    </item>
    <item>
      <title>SoccerNet-Caption: Dense Video Captioning for Soccer Broadcasts Commentaries</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/soccernet-caption-dense-video-captioning-for-soccer-broadcasts-commentaries/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/soccernet-caption-dense-video-captioning-for-soccer-broadcasts-commentaries/</guid>
      <description>&lt;p&gt;Abstract not available.&lt;/p&gt;
&lt;div style=&#34;margin-top: 1rem; padding: 1rem; display: inline-block;&#34;&gt;
  &lt;a href=&#34;https://api.semanticscholar.org/CorpusID:258049025&#34; target=&#34;_blank&#34; style=&#34;background-color: #0d9bdc; color: white; padding: 10px 16px; margin-right: 8px; text-decoration: none; border-radius: 4px; font-weight: bold;&#34;&gt;
    Download Paper
  &lt;/a&gt;
  &lt;a href=&#34;bib/soccernet-caption-dense-video-captioning-for-soccer-broadcasts-commentaries.bib&#34; download style=&#34;background-color: #f0a500; color: white; padding: 10px 16px; text-decoration: none; border-radius: 4px; font-weight: bold;&#34;&gt;
    Download BibTeX
  &lt;/a&gt;
&lt;/div&gt;</description>
    </item>
    <item>
      <title>SoccerNet-Echoes: A Soccer Game Audio Commentary Dataset</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/soccernet-echoes-a-soccer-game-audio-commentary-dataset/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/soccernet-echoes-a-soccer-game-audio-commentary-dataset/</guid>
      <description>&lt;p&gt;Abstract not available.&lt;/p&gt;
&lt;div style=&#34;margin-top: 1rem; padding: 1rem; display: inline-block;&#34;&gt;
  &lt;a href=&#34;https://api.semanticscholar.org/CorpusID:269757092&#34; target=&#34;_blank&#34; style=&#34;background-color: #0d9bdc; color: white; padding: 10px 16px; margin-right: 8px; text-decoration: none; border-radius: 4px; font-weight: bold;&#34;&gt;
    Download Paper
  &lt;/a&gt;
  &lt;a href=&#34;bib/soccernet-echoes-a-soccer-game-audio-commentary-dataset.bib&#34; download style=&#34;background-color: #f0a500; color: white; padding: 10px 16px; text-decoration: none; border-radius: 4px; font-weight: bold;&#34;&gt;
    Download BibTeX
  &lt;/a&gt;
&lt;/div&gt;</description>
    </item>
    <item>
      <title>SoccerNet-v2: A Dataset and Benchmarks for Holistic Understanding of Broadcast Soccer Videos</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/soccernet-v2-a-dataset-and-benchmarks-for-holistic-understanding-of-broadcast-soccer-videos/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/soccernet-v2-a-dataset-and-benchmarks-for-holistic-understanding-of-broadcast-soccer-videos/</guid>
      <description>&lt;p&gt;Abstract not available.&lt;/p&gt;
&lt;div style=&#34;margin-top: 1rem; padding: 1rem; display: inline-block;&#34;&gt;
  &lt;a href=&#34;https://doi.org/10.1109/CVPRW53098.2021.00508&#34; target=&#34;_blank&#34; style=&#34;background-color: #0d9bdc; color: white; padding: 10px 16px; margin-right: 8px; text-decoration: none; border-radius: 4px; font-weight: bold;&#34;&gt;
    Download Paper
  &lt;/a&gt;
  &lt;a href=&#34;bib/soccernet-v2-a-dataset-and-benchmarks-for-holistic-understanding-of-broadcast-soccer-videos.bib&#34; download style=&#34;background-color: #f0a500; color: white; padding: 10px 16px; text-decoration: none; border-radius: 4px; font-weight: bold;&#34;&gt;
    Download BibTeX
  &lt;/a&gt;
&lt;/div&gt;</description>
    </item>
    <item>
      <title>SoccerNet: A Scalable Dataset for Action Spotting in Soccer Videos</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/soccernet-a-scalable-dataset-for-action-spotting-in-soccer-videos/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/soccernet-a-scalable-dataset-for-action-spotting-in-soccer-videos/</guid>
      <description>&lt;p&gt;Abstract not available.&lt;/p&gt;
&lt;div style=&#34;margin-top: 1rem; padding: 1rem; display: inline-block;&#34;&gt;
  &lt;a href=&#34;https://doi.org/10.1109/cvprw.2018.00223&#34; target=&#34;_blank&#34; style=&#34;background-color: #0d9bdc; color: white; padding: 10px 16px; margin-right: 8px; text-decoration: none; border-radius: 4px; font-weight: bold;&#34;&gt;
    Download Paper
  &lt;/a&gt;
  &lt;a href=&#34;bib/soccernet-a-scalable-dataset-for-action-spotting-in-soccer-videos.bib&#34; download style=&#34;background-color: #f0a500; color: white; padding: 10px 16px; text-decoration: none; border-radius: 4px; font-weight: bold;&#34;&gt;
    Download BibTeX
  &lt;/a&gt;
&lt;/div&gt;</description>
    </item>
    <item>
      <title>Sports Commentary Recommendation System (SCoReS): machine learning for automated narrative</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/sports-commentary-recommendation-system-scores-machine-learning-for-automated-narrative/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/sports-commentary-recommendation-system-scores-machine-learning-for-automated-narrative/</guid>
      <description>&lt;p&gt;Automated sports commentary is a form of automated narrative. Sports commentary exists to keep the viewer informed and entertained. One way to entertain the viewer is by telling brief stories relevant to the game in progress. We introduce a system called the Sports Commentary Recommendation System (SCoReS) that can automatically suggest stories for commentators to tell during games. Through several user studies, we compared commentary using SCoReS to three other types of commentary and show that SCoReS adds significantly to the broadcast across several enjoyment metrics. We also collected interview data from professional sports commentators who positively evaluated a demonstration of the system. We conclude that SCoReS can be a useful broadcast tool, effective at selecting stories that add to the enjoyment and watch-ability of sports. SCoReS is a step toward automating sports commentary and, thus, automating narrative.&lt;/p&gt;</description>
    </item>
    <item>
      <title>STORY SELECTION AND RECOMMENDATION SYSTEM FOR COLOUR COMMENTARY IN CRICKET</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/story-selection-and-recommendation-system-for-colour-commentary-in-cricket/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/story-selection-and-recommendation-system-for-colour-commentary-in-cricket/</guid>
      <description>&lt;p&gt;Abstract not available.&lt;/p&gt;
&lt;div style=&#34;margin-top: 1rem; padding: 1rem; display: inline-block;&#34;&gt;
  &lt;a href=&#34;https://api.semanticscholar.org/CorpusID:212473004&#34; target=&#34;_blank&#34; style=&#34;background-color: #0d9bdc; color: white; padding: 10px 16px; margin-right: 8px; text-decoration: none; border-radius: 4px; font-weight: bold;&#34;&gt;
    Download Paper
  &lt;/a&gt;
  &lt;a href=&#34;bib/story-selection-and-recommendation-system-for-colour-commentary-in-cricket.bib&#34; download style=&#34;background-color: #f0a500; color: white; padding: 10px 16px; text-decoration: none; border-radius: 4px; font-weight: bold;&#34;&gt;
    Download BibTeX
  &lt;/a&gt;
&lt;/div&gt;</description>
    </item>
    <item>
      <title>TenniSet: A Dataset for Dense Fine-Grained Event Recognition, Localisation and Description</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/tenniset-a-dataset-for-dense-fine-grained-event-recognition-localisation-and-description/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/tenniset-a-dataset-for-dense-fine-grained-event-recognition-localisation-and-description/</guid>
      <description>&lt;p&gt;Abstract not available.&lt;/p&gt;
&lt;div style=&#34;margin-top: 1rem; padding: 1rem; display: inline-block;&#34;&gt;
  &lt;a href=&#34;https://doi.org/10.1109/DICTA.2017.8227494&#34; target=&#34;_blank&#34; style=&#34;background-color: #0d9bdc; color: white; padding: 10px 16px; margin-right: 8px; text-decoration: none; border-radius: 4px; font-weight: bold;&#34;&gt;
    Download Paper
  &lt;/a&gt;
  &lt;a href=&#34;bib/tenniset-a-dataset-for-dense-fine-grained-event-recognition-localisation-and-description.bib&#34; download style=&#34;background-color: #f0a500; color: white; padding: 10px 16px; text-decoration: none; border-radius: 4px; font-weight: bold;&#34;&gt;
    Download BibTeX
  &lt;/a&gt;
&lt;/div&gt;</description>
    </item>
    <item>
      <title>Training a Multilingual Sportscaster: Using Perceptual Context to Learn Language</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/training-a-multilingual-sportscaster-using-perceptual-context-to-learn-language/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/training-a-multilingual-sportscaster-using-perceptual-context-to-learn-language/</guid>
      <description>&lt;p&gt;Abstract not available.&lt;/p&gt;
&lt;div style=&#34;margin-top: 1rem; padding: 1rem; display: inline-block;&#34;&gt;
  &lt;a href=&#34;https://api.semanticscholar.org/CorpusID:6678765&#34; target=&#34;_blank&#34; style=&#34;background-color: #0d9bdc; color: white; padding: 10px 16px; margin-right: 8px; text-decoration: none; border-radius: 4px; font-weight: bold;&#34;&gt;
    Download Paper
  &lt;/a&gt;
  &lt;a href=&#34;bib/training-a-multilingual-sportscaster-using-perceptual-context-to-learn-language.bib&#34; download style=&#34;background-color: #f0a500; color: white; padding: 10px 16px; text-decoration: none; border-radius: 4px; font-weight: bold;&#34;&gt;
    Download BibTeX
  &lt;/a&gt;
&lt;/div&gt;</description>
    </item>
    <item>
      <title>Understanding Game-Playing Agents with Natural Language Annotations</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/understanding-game-playing-agents-with-natural-language-annotations/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/understanding-game-playing-agents-with-natural-language-annotations/</guid>
      <description>&lt;p&gt;Abstract not available.&lt;/p&gt;
&lt;div style=&#34;margin-top: 1rem; padding: 1rem; display: inline-block;&#34;&gt;
  &lt;a href=&#34;https://api.semanticscholar.org/CorpusID:248218465&#34; target=&#34;_blank&#34; style=&#34;background-color: #0d9bdc; color: white; padding: 10px 16px; margin-right: 8px; text-decoration: none; border-radius: 4px; font-weight: bold;&#34;&gt;
    Download Paper
  &lt;/a&gt;
  &lt;a href=&#34;bib/understanding-game-playing-agents-with-natural-language-annotations.bib&#34; download style=&#34;background-color: #f0a500; color: white; padding: 10px 16px; text-decoration: none; border-radius: 4px; font-weight: bold;&#34;&gt;
    Download BibTeX
  &lt;/a&gt;
&lt;/div&gt;</description>
    </item>
  </channel>
</rss>
