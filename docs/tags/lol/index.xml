<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>LoL on Datasheet for Game Commentary Datasets</title><link>https://ZzZqr.github.io/AIGGC-datasheet/tags/lol/</link><description>Recent content in LoL on Datasheet for Game Commentary Datasets</description><generator>Hugo -- 0.147.2</generator><language>en-us</language><atom:link href="https://ZzZqr.github.io/AIGGC-datasheet/tags/lol/index.xml" rel="self" type="application/rss+xml"/><item><title>Commentary Generation from Data Records of Multiplayer Strategy Esports Game</title><link>https://ZzZqr.github.io/AIGGC-datasheet/datasets/commentary-generation-from-data-records-of-multiplayer-strategy-esports-game/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ZzZqr.github.io/AIGGC-datasheet/datasets/commentary-generation-from-data-records-of-multiplayer-strategy-esports-game/</guid><description>&lt;p>Esports, a sports competition on video games, has become one of the most important sporting events. Although esports play logs have been accumulated, only a small portion of them accompany text commentaries for the audience to retrieve and understand the plays. In this study, we therefore introduce the task of generating game commentaries from esports&amp;rsquo; data records. We first build large-scale esports data-to-text datasets that pair structured data and commentaries from a popular esports game, League of Legends. We then evaluate Transformer-based models to generate game commentaries from structured data records, while examining the impact of the pre-trained language models. Evaluation results on our dataset revealed the challenges of this novel task. We will release our dataset to boost potential research in the data-to-text generation community.&lt;/p></description></item><item><title>CS-lol: a Dataset of Viewer Comment with Scene in E-sports Live-streaming</title><link>https://ZzZqr.github.io/AIGGC-datasheet/datasets/cs-lol-a-dataset-of-viewer-comment-with-scene-in-e-sports-live-streaming/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ZzZqr.github.io/AIGGC-datasheet/datasets/cs-lol-a-dataset-of-viewer-comment-with-scene-in-e-sports-live-streaming/</guid><description>&lt;p>Billions of live-streaming viewers share their opinions on scenes they are watching in real-time and interact with the event, commentators as well as other viewers via text comments. Thus, there is necessary to explore viewers’ comments with scenes in E-sport live-streaming events. In this paper, we developed CS-lol, a new large-scale dataset containing comments from viewers paired with descriptions of game scenes in E-sports live-streaming. Moreover, we propose a task, namely viewer comment retrieval, to retrieve the viewer comments for the scene of the live-streaming event. Results on a series of baseline retrieval methods derived from typical IR evaluation methods show our task as a challenging task. Finally, we release CS-lol and baseline implementation to the research community as a resource.&lt;/p></description></item><item><title>From eSports Data to Game Commentary: Datasets, Models, and Evaluation Metrics</title><link>https://ZzZqr.github.io/AIGGC-datasheet/datasets/from-esports-data-to-game-commentary-datasets-models-and-evaluation-metrics/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ZzZqr.github.io/AIGGC-datasheet/datasets/from-esports-data-to-game-commentary-datasets-models-and-evaluation-metrics/</guid><description>&lt;p>Electronic sports (eSports), the sport competition using video games, has become one of the most popular sporting events now. The eSports audience needs textual commentaries for deeply understanding the games and for eﬃciently retrieving speciﬁc games of their interest. Therefore, in this work, we set up an eSports data-to-text generation task and tackle three fundamental problems: dataset construction, model design, and evaluation metrics. We ﬁrst build a data-to-text dataset containing data records and game commentaries from the a popular eSports game, League of Legends. On this new dataset, we propose a hierarchical model to address diﬃculty in handling long sequences of inputs and outputs with an encoder-decoder model. The hierarchical model sets multi-level encoders for the input data. Besides, we organize and design a new set of evaluation metrics including three aspects to meet this task’s goal. Experimental results on the new datasets conﬁrm that the hierarchical structure improves the performance of the model.&lt;/p></description></item><item><title>Game-MUG: Multimodal Oriented Game Situation Understanding and Commentary Generation Dataset</title><link>https://ZzZqr.github.io/AIGGC-datasheet/datasets/game-mug-multimodal-oriented-game-situation-understanding-and-commentary-generation-dataset/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ZzZqr.github.io/AIGGC-datasheet/datasets/game-mug-multimodal-oriented-game-situation-understanding-and-commentary-generation-dataset/</guid><description>&lt;p>The dynamic nature of esports makes the situation relatively complicated for average viewers. Esports broadcasting involves game expert casters, but the caster-dependent game commentary is not enough to fully understand the game situation. It will be richer by including diverse multimodal esports information, including audiences&amp;rsquo; talks/emotions, game audio, and game match event information. This paper introduces GAME-MUG, a new multimodal game situation understanding and audience-engaged commentary generation dataset and its strong baseline. Our dataset is collected from 2020-2022 LOL game live streams from YouTube and Twitch, and includes multimodal esports game information, including text, audio, and time-series event logs, for detecting the game situation. In addition, we also propose a new audience conversation augmented commentary dataset by covering the game situation and audience conversation understanding, and introducing a robust joint multimodal dual learning model as a baseline. We examine the model&amp;rsquo;s game situation/event understanding ability and commentary generation capability to show the effectiveness of the multimodal aspects coverage and the joint integration learning approach.&lt;/p></description></item><item><title>LoL-V2T: Large-Scale Esports Video Description Dataset</title><link>https://ZzZqr.github.io/AIGGC-datasheet/datasets/lol-v2t-large-scale-esports-video-description-dataset/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ZzZqr.github.io/AIGGC-datasheet/datasets/lol-v2t-large-scale-esports-video-description-dataset/</guid><description>&lt;p>Esports is a fastest-growing new field with a largely online-presence, and is creating a demand for automatic domain-specific captioning tools. However, at the current time, there are few approaches that tackle the esports video description problem. In this work, we propose a large-scale dataset for esports video description, focusing on the popular game &amp;ldquo;League of Legends&amp;rdquo;. The dataset, which we call LoL-V2T, is the largest video description dataset in the video game domain, and includes 9,723 clips with 62,677 captions. This new dataset presents multiple new video captioning challenges such as large amounts of domain-specific vocabulary, subtle motions with large importance, and a temporal gap between most captions and the events that occurred. In order to tackle the issue of vocabulary, we propose a masking the domain-specific words and provide additional annotations for this. In our results, we show that the dataset poses a challenge to existing video captioning approaches, and the masking can significantly improve performance. Our dataset and code is publicly available1.&lt;/p></description></item><item><title>Multimodal Joint Emotion and Game Context Recognition in League of Legends Livestreams</title><link>https://ZzZqr.github.io/AIGGC-datasheet/datasets/multimodal-joint-emotion-and-game-context-recognition-in-league-of-legends-livestreams/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ZzZqr.github.io/AIGGC-datasheet/datasets/multimodal-joint-emotion-and-game-context-recognition-in-league-of-legends-livestreams/</guid><description>&lt;p>Video game streaming provides the viewer with a rich set of audio-visual data, conveying information both with regards to the game itself, through game footage and audio, as well as the streamer&amp;rsquo;s emotional state and behaviour via webcam footage and audio. Analysing player behaviour and discovering correlations with game context is crucial for modelling and understanding important aspects of livestreams, but comes with a significant set of challenges - such as fusing multimodal data captured by different sensors in uncontrolled (`in-the-wild&amp;rsquo;) conditions. Firstly, we present, to our knowledge, the first data set of League of Legends livestreams, annotated for both streamer affect and game context. Secondly, we propose a method that exploits tensor decompositions for high-order fusion of multimodal representations. The proposed method is evaluated on the problem of jointly predicting game context and player affect, compared with a set of baseline fusion approaches such as late and early fusion. Data and code are available at &lt;a href="https://github.com/charlieringer/LoLEmoGameRecognition">https://github.com/charlieringer/LoLEmoGameRecognition&lt;/a>.&lt;/p></description></item></channel></rss>