<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/"><channel><title>Esports on Datasheet for Game Commentary Datasets</title><link>https://ZzZqr.github.io/AIGGC-datasheet/tags/esports/</link><description>Recent content in Esports on Datasheet for Game Commentary Datasets</description><generator>Hugo -- 0.147.2</generator><language>en-us</language><atom:link href="https://ZzZqr.github.io/AIGGC-datasheet/tags/esports/index.xml" rel="self" type="application/rss+xml"/><item><title>Audio Commentary System for Real-Time Racing Game Play</title><link>https://ZzZqr.github.io/AIGGC-datasheet/datasets/audio-commentary-system-for-real-time-racing-game-play/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ZzZqr.github.io/AIGGC-datasheet/datasets/audio-commentary-system-for-real-time-racing-game-play/</guid><description>&lt;p>Live commentaries are essential for enhancing spectators&amp;rsquo; enjoyment and understanding during sports events or e-sports streams. We introduce a live audio commentator system designed specifically for a racing game, driven by the high demand in the e-sports field. While a player is playing a racing game, our system tracks real-time user play data including speed and steer rotations, and generates commentary to accompany the live stream. Human evaluation suggested that generated commentary enhances enjoyment and understanding of races compared to streams without commentary. Incorporating additional modules to improve diversity and detect irregular events, such as course-outs and collisions, further increases the preference for the output commentaries.&lt;/p></description></item><item><title>Commentary Generation from Data Records of Multiplayer Strategy Esports Game</title><link>https://ZzZqr.github.io/AIGGC-datasheet/datasets/commentary-generation-from-data-records-of-multiplayer-strategy-esports-game/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ZzZqr.github.io/AIGGC-datasheet/datasets/commentary-generation-from-data-records-of-multiplayer-strategy-esports-game/</guid><description>&lt;p>Esports, a sports competition on video games, has become one of the most important sporting events. Although esports play logs have been accumulated, only a small portion of them accompany text commentaries for the audience to retrieve and understand the plays. In this study, we therefore introduce the task of generating game commentaries from esports&amp;rsquo; data records. We first build large-scale esports data-to-text datasets that pair structured data and commentaries from a popular esports game, League of Legends. We then evaluate Transformer-based models to generate game commentaries from structured data records, while examining the impact of the pre-trained language models. Evaluation results on our dataset revealed the challenges of this novel task. We will release our dataset to boost potential research in the data-to-text generation community.&lt;/p></description></item><item><title>Conceptual Representation and Evaluation of an FPS Game Commentary Generator</title><link>https://ZzZqr.github.io/AIGGC-datasheet/datasets/conceptual-representation-and-evaluation-of-an-fps-game-commentary-generator/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ZzZqr.github.io/AIGGC-datasheet/datasets/conceptual-representation-and-evaluation-of-an-fps-game-commentary-generator/</guid><description>&lt;p>Playing video games has been popular across all the age limits of modern society. In the beginning, it was limited among the younger community and it was just a hobby limited to individuals. Even though the majority of society sees video gaming as having a negative impact on society, this modern industry acts a significant role in healing the present competitive, stressful society. Game commentary has played a major role in the domain of competitive ESports. A proper game commentary is beneficial to both players and the audience. The aim of this project is to analyze the gameplays to produce a commentary track while balancing the contributing factors, color commentary, and play-by-play commentary. The project consists of three modules that perform the study in three perspectives: 1. Word sets related to action, spatial, temporal, and statistical information, 2. Word sets related to color commentary, 3. Word sets related to play-by-play commentary. In each module, a game commentary is generated using only the word sets related to that module. For evaluation, the similarity between the human commentary and the generated commentaries individually will be calculated.&lt;/p></description></item><item><title>CS-lol: a Dataset of Viewer Comment with Scene in E-sports Live-streaming</title><link>https://ZzZqr.github.io/AIGGC-datasheet/datasets/cs-lol-a-dataset-of-viewer-comment-with-scene-in-e-sports-live-streaming/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ZzZqr.github.io/AIGGC-datasheet/datasets/cs-lol-a-dataset-of-viewer-comment-with-scene-in-e-sports-live-streaming/</guid><description>&lt;p>Billions of live-streaming viewers share their opinions on scenes they are watching in real-time and interact with the event, commentators as well as other viewers via text comments. Thus, there is necessary to explore viewers’ comments with scenes in E-sport live-streaming events. In this paper, we developed CS-lol, a new large-scale dataset containing comments from viewers paired with descriptions of game scenes in E-sports live-streaming. Moreover, we propose a task, namely viewer comment retrieval, to retrieve the viewer comments for the scene of the live-streaming event. Results on a series of baseline retrieval methods derived from typical IR evaluation methods show our task as a challenging task. Finally, we release CS-lol and baseline implementation to the research community as a resource.&lt;/p></description></item><item><title>From eSports Data to Game Commentary: Datasets, Models, and Evaluation Metrics</title><link>https://ZzZqr.github.io/AIGGC-datasheet/datasets/from-esports-data-to-game-commentary-datasets-models-and-evaluation-metrics/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ZzZqr.github.io/AIGGC-datasheet/datasets/from-esports-data-to-game-commentary-datasets-models-and-evaluation-metrics/</guid><description>&lt;p>Electronic sports (eSports), the sport competition using video games, has become one of the most popular sporting events now. The eSports audience needs textual commentaries for deeply understanding the games and for eﬃciently retrieving speciﬁc games of their interest. Therefore, in this work, we set up an eSports data-to-text generation task and tackle three fundamental problems: dataset construction, model design, and evaluation metrics. We ﬁrst build a data-to-text dataset containing data records and game commentaries from the a popular eSports game, League of Legends. On this new dataset, we propose a hierarchical model to address diﬃculty in handling long sequences of inputs and outputs with an encoder-decoder model. The hierarchical model sets multi-level encoders for the input data. Besides, we organize and design a new set of evaluation metrics including three aspects to meet this task’s goal. Experimental results on the new datasets conﬁrm that the hierarchical structure improves the performance of the model.&lt;/p></description></item><item><title>Game-MUG: Multimodal Oriented Game Situation Understanding and Commentary Generation Dataset</title><link>https://ZzZqr.github.io/AIGGC-datasheet/datasets/game-mug-multimodal-oriented-game-situation-understanding-and-commentary-generation-dataset/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ZzZqr.github.io/AIGGC-datasheet/datasets/game-mug-multimodal-oriented-game-situation-understanding-and-commentary-generation-dataset/</guid><description>&lt;p>The dynamic nature of esports makes the situation relatively complicated for average viewers. Esports broadcasting involves game expert casters, but the caster-dependent game commentary is not enough to fully understand the game situation. It will be richer by including diverse multimodal esports information, including audiences&amp;rsquo; talks/emotions, game audio, and game match event information. This paper introduces GAME-MUG, a new multimodal game situation understanding and audience-engaged commentary generation dataset and its strong baseline. Our dataset is collected from 2020-2022 LOL game live streams from YouTube and Twitch, and includes multimodal esports game information, including text, audio, and time-series event logs, for detecting the game situation. In addition, we also propose a new audience conversation augmented commentary dataset by covering the game situation and audience conversation understanding, and introducing a robust joint multimodal dual learning model as a baseline. We examine the model&amp;rsquo;s game situation/event understanding ability and commentary generation capability to show the effectiveness of the multimodal aspects coverage and the joint integration learning approach.&lt;/p></description></item><item><title>Generating Racing Game Commentary from Vision, Language, and Structured Data</title><link>https://ZzZqr.github.io/AIGGC-datasheet/datasets/generating-racing-game-commentary-from-vision-language-and-structured-data/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ZzZqr.github.io/AIGGC-datasheet/datasets/generating-racing-game-commentary-from-vision-language-and-structured-data/</guid><description>&lt;p>We propose the task of automatically generating commentaries for races in a motor racing game, from vision, structured numerical, and textual data. Commentaries provide information to support spectators in understanding events in races. Commentary generation models need to interpret the race situation and generate the correct content at the right moment. We divide the task into two subtasks: utterance timing identification and utterance generation. Because existing datasets do not have such alignments of data in multiple modalities, this setting has not been explored in depth. In this study, we introduce a new large-scale dataset that contains aligned video data, structured numerical data, and transcribed commentaries that consist of 129,226 utterances in 1,389 races in a game. Our analysis reveals that the characteristics of commentaries change over time or from viewpoints. Our experiments on the subtasks show that it is still challenging for a state-of-the-art vision encoder to capture useful information from videos to generate accurate commentaries. We make the dataset and baseline implementation publicly available for further research.&lt;/p></description></item><item><title>Learning to sportscast: a test of grounded language acquisition</title><link>https://ZzZqr.github.io/AIGGC-datasheet/datasets/learning-to-sportscast-a-test-of-grounded-language-acquisition/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ZzZqr.github.io/AIGGC-datasheet/datasets/learning-to-sportscast-a-test-of-grounded-language-acquisition/</guid><description>&lt;p>We present a novel commentator system that learns language from sportscasts of simulated soccer games. The system learns to parse and generate commentaries without any engineered knowledge about the English language. Training is done using only ambiguous supervision in the form of textual human commentaries and simulation states of the soccer games. The system simultaneously tries to establish correspondences between the commentaries and the simulation states as well as build a translation model. We also present a novel algorithm, Iterative Generation Strategy Learning (IGSL), for deciding which events to comment on. Human evaluations of the generated commentaries indicate they are of reasonable quality compared to human commentaries.&lt;/p></description></item><item><title>LoL-V2T: Large-Scale Esports Video Description Dataset</title><link>https://ZzZqr.github.io/AIGGC-datasheet/datasets/lol-v2t-large-scale-esports-video-description-dataset/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ZzZqr.github.io/AIGGC-datasheet/datasets/lol-v2t-large-scale-esports-video-description-dataset/</guid><description>&lt;p>Esports is a fastest-growing new field with a largely online-presence, and is creating a demand for automatic domain-specific captioning tools. However, at the current time, there are few approaches that tackle the esports video description problem. In this work, we propose a large-scale dataset for esports video description, focusing on the popular game &amp;ldquo;League of Legends&amp;rdquo;. The dataset, which we call LoL-V2T, is the largest video description dataset in the video game domain, and includes 9,723 clips with 62,677 captions. This new dataset presents multiple new video captioning challenges such as large amounts of domain-specific vocabulary, subtle motions with large importance, and a temporal gap between most captions and the events that occurred. In order to tackle the issue of vocabulary, we propose a masking the domain-specific words and provide additional annotations for this. In our results, we show that the dataset poses a challenge to existing video captioning approaches, and the masking can significantly improve performance. Our dataset and code is publicly available1.&lt;/p></description></item><item><title>MOBA-E2C: Generating MOBA Game Commentaries via Capturing Highlight Events from the Meta-Data</title><link>https://ZzZqr.github.io/AIGGC-datasheet/datasets/moba-e2c-generating-moba-game-commentaries-via-capturing-highlight-events-from-the-meta-data/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ZzZqr.github.io/AIGGC-datasheet/datasets/moba-e2c-generating-moba-game-commentaries-via-capturing-highlight-events-from-the-meta-data/</guid><description>&lt;p>MOBA (Multiplayer Online Battle Arena) games such as Dota2 are currently one of the most popular e-sports gaming genres. Following professional commentaries is a great way to understand and enjoy a MOBA game. However, massive game competitions lack commentaries because of the shortage of professional human commentators. As an alternative, employing machine commentators that can work at any time and place is a feasible solution. Considering the challenges in modeling MOBA games, we propose a data-driven MOBA commentary generation framework, MOBA-E2C, allowing a model to generate commentaries based on the game meta-data. Subsequently, to alleviate the burden of collecting supervised data, we propose a MOBA-FuseGPT generator to generate MOBA game commentaries by fusing the power of a rule-based generator and a generative GPT generator. Finally, in the experiments, we take a popular MOBA game Dota2 as our case and construct a Chinese Dota2 commentary generation dataset Dota2-Commentary. Experimental results demonstrate the superior performance of our approach. To the best of our knowledge, this work is the first Dota2 machine commentator and Dota2-Commentary is the first dataset.&lt;/p></description></item><item><title>Multimodal Joint Emotion and Game Context Recognition in League of Legends Livestreams</title><link>https://ZzZqr.github.io/AIGGC-datasheet/datasets/multimodal-joint-emotion-and-game-context-recognition-in-league-of-legends-livestreams/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ZzZqr.github.io/AIGGC-datasheet/datasets/multimodal-joint-emotion-and-game-context-recognition-in-league-of-legends-livestreams/</guid><description>&lt;p>Video game streaming provides the viewer with a rich set of audio-visual data, conveying information both with regards to the game itself, through game footage and audio, as well as the streamer&amp;rsquo;s emotional state and behaviour via webcam footage and audio. Analysing player behaviour and discovering correlations with game context is crucial for modelling and understanding important aspects of livestreams, but comes with a significant set of challenges - such as fusing multimodal data captured by different sensors in uncontrolled (`in-the-wild&amp;rsquo;) conditions. Firstly, we present, to our knowledge, the first data set of League of Legends livestreams, annotated for both streamer affect and game context. Secondly, we propose a method that exploits tensor decompositions for high-order fusion of multimodal representations. The proposed method is evaluated on the problem of jointly predicting game context and player affect, compared with a set of baseline fusion approaches such as late and early fusion. Data and code are available at &lt;a href="https://github.com/charlieringer/LoLEmoGameRecognition">https://github.com/charlieringer/LoLEmoGameRecognition&lt;/a>.&lt;/p></description></item><item><title>Training a Multilingual Sportscaster: Using Perceptual Context to Learn Language</title><link>https://ZzZqr.github.io/AIGGC-datasheet/datasets/training-a-multilingual-sportscaster-using-perceptual-context-to-learn-language/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ZzZqr.github.io/AIGGC-datasheet/datasets/training-a-multilingual-sportscaster-using-perceptual-context-to-learn-language/</guid><description>&lt;p>We present a novel framework for learning to interpret and generate language using only perceptual context as supervision. We demonstrate its capabilities by developing a system that learns to sportscast simulated robot soccer games in both English and Korean without any language-specific prior knowledge. Training employs only ambiguous supervision consisting of a stream of descriptive textual comments and a sequence of events extracted from the simulation trace. The system simultaneously establishes correspondences between individual comments and the events that they describe while building a translation model that supports both parsing and generation. We also present a novel algorithm for learning which events are worth describing. Human evaluations of the generated commentaries indicate they are of reasonable quality and in some cases even on par with those produced by humans for our limited domain.&lt;/p></description></item></channel></rss>