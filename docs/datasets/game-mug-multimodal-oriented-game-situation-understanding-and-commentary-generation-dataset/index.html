<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Game-MUG: Multimodal Oriented Game Situation Understanding and Commentary Generation Dataset | Datasheet for Game Commentary Datasets</title>
<meta name=keywords content="Esports,LoL"><meta name=description content="The dynamic nature of esports makes the situation relatively complicated for average viewers. Esports broadcasting involves game expert casters, but the caster-dependent game commentary is not enough to fully understand the game situation. It will be richer by including diverse multimodal esports information, including audiences&rsquo; talks/emotions, game audio, and game match event information. This paper introduces GAME-MUG, a new multimodal game situation understanding and audience-engaged commentary generation dataset and its strong baseline. Our dataset is collected from 2020-2022 LOL game live streams from YouTube and Twitch, and includes multimodal esports game information, including text, audio, and time-series event logs, for detecting the game situation. In addition, we also propose a new audience conversation augmented commentary dataset by covering the game situation and audience conversation understanding, and introducing a robust joint multimodal dual learning model as a baseline. We examine the model&rsquo;s game situation/event understanding ability and commentary generation capability to show the effectiveness of the multimodal aspects coverage and the joint integration learning approach."><meta name=author content><link rel=canonical href=https://ZzZqr.github.io/AIGGC-datasheet/datasets/game-mug-multimodal-oriented-game-situation-understanding-and-commentary-generation-dataset/><link crossorigin=anonymous href=/AIGGC-datasheet/assets/css/stylesheet.6e3cae23ffdb771f23d63cb0f916b137936e5c06b0eed3ec4bc79a2b0809b644.css integrity rel="preload stylesheet" as=style><link rel=icon href=https://ZzZqr.github.io/AIGGC-datasheet/favicon.ico><link rel=icon type=image/png sizes=16x16 href=https://ZzZqr.github.io/AIGGC-datasheet/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=https://ZzZqr.github.io/AIGGC-datasheet/favicon-32x32.png><link rel=apple-touch-icon href=https://ZzZqr.github.io/AIGGC-datasheet/apple-touch-icon.png><link rel=mask-icon href=https://ZzZqr.github.io/AIGGC-datasheet/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=https://ZzZqr.github.io/AIGGC-datasheet/datasets/game-mug-multimodal-oriented-game-situation-understanding-and-commentary-generation-dataset/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><meta property="og:url" content="https://ZzZqr.github.io/AIGGC-datasheet/datasets/game-mug-multimodal-oriented-game-situation-understanding-and-commentary-generation-dataset/"><meta property="og:site_name" content="Datasheet for Game Commentary Datasets"><meta property="og:title" content="Game-MUG: Multimodal Oriented Game Situation Understanding and Commentary Generation Dataset"><meta property="og:description" content="The dynamic nature of esports makes the situation relatively complicated for average viewers. Esports broadcasting involves game expert casters, but the caster-dependent game commentary is not enough to fully understand the game situation. It will be richer by including diverse multimodal esports information, including audiences’ talks/emotions, game audio, and game match event information. This paper introduces GAME-MUG, a new multimodal game situation understanding and audience-engaged commentary generation dataset and its strong baseline. Our dataset is collected from 2020-2022 LOL game live streams from YouTube and Twitch, and includes multimodal esports game information, including text, audio, and time-series event logs, for detecting the game situation. In addition, we also propose a new audience conversation augmented commentary dataset by covering the game situation and audience conversation understanding, and introducing a robust joint multimodal dual learning model as a baseline. We examine the model’s game situation/event understanding ability and commentary generation capability to show the effectiveness of the multimodal aspects coverage and the joint integration learning approach."><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="datasets"><meta property="article:tag" content="Esports"><meta property="article:tag" content="LoL"><meta name=twitter:card content="summary"><meta name=twitter:title content="Game-MUG: Multimodal Oriented Game Situation Understanding and Commentary Generation Dataset"><meta name=twitter:description content="The dynamic nature of esports makes the situation relatively complicated for average viewers. Esports broadcasting involves game expert casters, but the caster-dependent game commentary is not enough to fully understand the game situation. It will be richer by including diverse multimodal esports information, including audiences&rsquo; talks/emotions, game audio, and game match event information. This paper introduces GAME-MUG, a new multimodal game situation understanding and audience-engaged commentary generation dataset and its strong baseline. Our dataset is collected from 2020-2022 LOL game live streams from YouTube and Twitch, and includes multimodal esports game information, including text, audio, and time-series event logs, for detecting the game situation. In addition, we also propose a new audience conversation augmented commentary dataset by covering the game situation and audience conversation understanding, and introducing a robust joint multimodal dual learning model as a baseline. We examine the model&rsquo;s game situation/event understanding ability and commentary generation capability to show the effectiveness of the multimodal aspects coverage and the joint integration learning approach."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Game Commentary Datasets","item":"https://ZzZqr.github.io/AIGGC-datasheet/datasets/"},{"@type":"ListItem","position":2,"name":"Game-MUG: Multimodal Oriented Game Situation Understanding and Commentary Generation Dataset","item":"https://ZzZqr.github.io/AIGGC-datasheet/datasets/game-mug-multimodal-oriented-game-situation-understanding-and-commentary-generation-dataset/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Game-MUG: Multimodal Oriented Game Situation Understanding and Commentary Generation Dataset","name":"Game-MUG: Multimodal Oriented Game Situation Understanding and Commentary Generation Dataset","description":"The dynamic nature of esports makes the situation relatively complicated for average viewers. Esports broadcasting involves game expert casters, but the caster-dependent game commentary is not enough to fully understand the game situation. It will be richer by including diverse multimodal esports information, including audiences\u0026rsquo; talks/emotions, game audio, and game match event information. This paper introduces GAME-MUG, a new multimodal game situation understanding and audience-engaged commentary generation dataset and its strong baseline. Our dataset is collected from 2020-2022 LOL game live streams from YouTube and Twitch, and includes multimodal esports game information, including text, audio, and time-series event logs, for detecting the game situation. In addition, we also propose a new audience conversation augmented commentary dataset by covering the game situation and audience conversation understanding, and introducing a robust joint multimodal dual learning model as a baseline. We examine the model\u0026rsquo;s game situation/event understanding ability and commentary generation capability to show the effectiveness of the multimodal aspects coverage and the joint integration learning approach.\n","keywords":["Esports","LoL"],"articleBody":"The dynamic nature of esports makes the situation relatively complicated for average viewers. Esports broadcasting involves game expert casters, but the caster-dependent game commentary is not enough to fully understand the game situation. It will be richer by including diverse multimodal esports information, including audiences’ talks/emotions, game audio, and game match event information. This paper introduces GAME-MUG, a new multimodal game situation understanding and audience-engaged commentary generation dataset and its strong baseline. Our dataset is collected from 2020-2022 LOL game live streams from YouTube and Twitch, and includes multimodal esports game information, including text, audio, and time-series event logs, for detecting the game situation. In addition, we also propose a new audience conversation augmented commentary dataset by covering the game situation and audience conversation understanding, and introducing a robust joint multimodal dual learning model as a baseline. We examine the model’s game situation/event understanding ability and commentary generation capability to show the effectiveness of the multimodal aspects coverage and the joint integration learning approach.\nDownload Paper\rDownload BibTeX\r","wordCount":"168","inLanguage":"en","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","mainEntityOfPage":{"@type":"WebPage","@id":"https://ZzZqr.github.io/AIGGC-datasheet/datasets/game-mug-multimodal-oriented-game-situation-understanding-and-commentary-generation-dataset/"},"publisher":{"@type":"Organization","name":"Datasheet for Game Commentary Datasets","logo":{"@type":"ImageObject","url":"https://ZzZqr.github.io/AIGGC-datasheet/favicon.ico"}}}</script></head><body id=top><header class=header><nav class=nav><div class=logo><a href=https://ZzZqr.github.io/AIGGC-datasheet/ accesskey=h title="Datasheet for Game Commentary Datasets (Alt + H)">Datasheet for Game Commentary Datasets</a><div class=logo-switches></div></div><ul id=menu><li><a href=https://ZzZqr.github.io/AIGGC-datasheet/datasets/ title=Datasets><span>Datasets</span></a></li><li><a href=https://ZzZqr.github.io/AIGGC-datasheet/datasheet/ title=Datasheet><span>Datasheet</span></a></li><li><a href=https://ZzZqr.github.io/AIGGC-datasheet/about/ title=About><span>About</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class="post-title entry-hint-parent">Game-MUG: Multimodal Oriented Game Situation Understanding and Commentary Generation Dataset</h1><div class=post-meta></div></header><div class=post-content><p>The dynamic nature of esports makes the situation relatively complicated for average viewers. Esports broadcasting involves game expert casters, but the caster-dependent game commentary is not enough to fully understand the game situation. It will be richer by including diverse multimodal esports information, including audiences&rsquo; talks/emotions, game audio, and game match event information. This paper introduces GAME-MUG, a new multimodal game situation understanding and audience-engaged commentary generation dataset and its strong baseline. Our dataset is collected from 2020-2022 LOL game live streams from YouTube and Twitch, and includes multimodal esports game information, including text, audio, and time-series event logs, for detecting the game situation. In addition, we also propose a new audience conversation augmented commentary dataset by covering the game situation and audience conversation understanding, and introducing a robust joint multimodal dual learning model as a baseline. We examine the model&rsquo;s game situation/event understanding ability and commentary generation capability to show the effectiveness of the multimodal aspects coverage and the joint integration learning approach.</p><div style=margin-top:1rem;padding:1rem;display:inline-block><a href=https://api.semanticscholar.org/CorpusID:269456978 target=_blank style="background-color:#0d9bdc;color:#fff;padding:10px 16px;margin-right:8px;text-decoration:none;border-radius:4px;font-weight:700">Download Paper
</a><a href=../bib/game-mug-multimodal-oriented-game-situation-understanding-and-commentary-generation-dataset.bib download style="background-color:#f0a500;color:#fff;padding:10px 16px;text-decoration:none;border-radius:4px;font-weight:700">Download BibTeX</a></div></div><footer class=post-footer><ul class=post-tags><li><a href=https://ZzZqr.github.io/AIGGC-datasheet/tags/esports/>Esports</a></li><li><a href=https://ZzZqr.github.io/AIGGC-datasheet/tags/lol/>LoL</a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=https://ZzZqr.github.io/AIGGC-datasheet/>Datasheet for Game Commentary Datasets</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script></body></html>