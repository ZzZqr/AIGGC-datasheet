<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Datasets on Datasheet for Game Commentary Datasets</title>
    <link>http://localhost:1313/AIGGC-datasheet/datasets/</link>
    <description>Recent content in Datasets on Datasheet for Game Commentary Datasets</description>
    <generator>Hugo -- 0.147.2</generator>
    <language>en-us</language>
    <atom:link href="http://localhost:1313/AIGGC-datasheet/datasets/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>A Descriptive Basketball Highlight Dataset for Automatic Commentary Generation</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/a-descriptive-basketball-highlight-dataset-for-automatic-commentary-generation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/a-descriptive-basketball-highlight-dataset-for-automatic-commentary-generation/</guid>
      <description>&lt;p&gt;The emergence of video captioning makes it possible to automatically generate natural language description for a given video. However, generating detailed video descriptions that incorporate domain-specific information remains an unsolved challenge, holding significant research and application value, particularly in domains such as sports commentary generation. Moreover, sports event commentary goes beyond being a mere game report, it involves entertaining, metaphorical, and emotional descriptions. To promote the field of sports commentary automatic generation, in this paper, we introduce a novel dataset, the Basketball Highlight Commentary (BH-Commentary), comprising approximately 4K basketball highlight videos with groundtruth commentaries from professional commentators. In addition, we propose an end-to-end framework as a benchmark for basketball highlight commentary generation task, in which a lightweight and effective prompt strategy is designed to enhance alignment fusion among visual and textual features. Experimental results on the BH-Commentary dataset demonstrate the validity of the dataset and the effectiveness of the proposed benchmark for sports highlight commentary generation.&lt;/p&gt;</description>
    </item>
    <item>
      <title>A Hybrid Deep Learning Model for Automated Cricket Commentary Generation</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/a-hybrid-deep-learning-model-for-automated-cricket-commentary-generation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/a-hybrid-deep-learning-model-for-automated-cricket-commentary-generation/</guid>
      <description>&lt;p&gt;The paper proposes an innovative method for generating cricket commentary. A hybrid model is proposed that combines three types of neural networks: Convolutional Neural Networks (CNN) for image processing, Long Short-Term Memory (LSTM) networks for sequential text generation, and Graph Convolutional Networks (GCN) for semantic understanding. By integrating these components, the model can generate ball-by-ball commentary that is coherent and contextaware. The model works by processing video frames from a cricket match using CNN. The resulting feature maps are used to retain essential visual information. Fully connected layers transform the features to a format suitable for input into the LSTM. The LSTM generates one word at a time, considering the temporal dependencies inherent in ball-by-ball events. To enhance the semantic understanding between the generated captions, the GCN is used. Evaluation metrics like BLEU, METEOR, and ROUGE are used to assess the proficiency of the model.&lt;/p&gt;</description>
    </item>
    <item>
      <title>A Japanese Chess Commentary Corpus</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/a-japanese-chess-commentary-corpus/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/a-japanese-chess-commentary-corpus/</guid>
      <description>&lt;p&gt;In recent years there has been a surge of interest in the natural language prosessing related to the real world, such as symbol grounding, language generation, and nonlinguistic data search by natural language queries. In order to concentrate on language ambiguities, we propose to use a well-defined {\textquotedblleft}real world,{\textquotedblright} that is game states. We built a corpus consisting of pairs of sentences and a game state. The game we focus on is shogi (Japanese chess). We collected 742,286 commentary sentences in Japanese. They are spontaneously generated contrary to natural language annotations in many image datasets provided by human workers on Amazon Mechanical Turk. We defined domain specific named entities and we segmented 2,508 sentences into words manually and annotated each word with a named entity tag. We describe a detailed definition of named entities and show some statistics of our game commentary corpus. We also show the results of the experiments of word segmentation and named entity recognition. The accuracies are as high as those on general domain texts indicating that we are ready to tackle various new problems related to the real world.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Annotating Event Appearance for Japanese Chess Commentary Corpus</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/annotating-event-appearance-for-japanese-chess-commentary-corpus/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/annotating-event-appearance-for-japanese-chess-commentary-corpus/</guid>
      <description>&lt;p&gt;In recent years, there has been a surge of interest in natural language processing related to the real world, such as symbol grounding, language generation, and non-linguistic data search by natural language queries. Researchers usually collect pairs of text and non-text data for research. However, the text and non-text data are not always a {\textquotedblleft}true{\textquotedblright} pair. We focused on the shogi (Japanese chess) commentaries, which are accompanied by game states as a well-defined {\textquotedblleft}real world{\textquotedblright}. For analyzing and processing texts accurately, considering only the given states is insufficient, and we must consider the relationship between texts and the real world. In this paper, we propose {\textquotedblleft}Event Appearance{\textquotedblright} labels that show the relationship between events mentioned in texts and those happening in the real world. Our event appearance label set consists of temporal relation, appearance probability, and evidence of the event. Statistics of the annotated corpus and the experimental result show that there exists temporal relation which skillful annotators realize in common. However, it is hard to predict the relationship only by considering the given states.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Annotating Modality Expressions and Event Factuality for a{Japanese Chess Commentary Corpus</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/annotating-modality-expressions-and-event-factuality-for-a-japanese-chess-commentary-corpus/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/annotating-modality-expressions-and-event-factuality-for-a-japanese-chess-commentary-corpus/</guid>
      <description>&lt;p&gt;In recent years, there has been a surge of interest in the natural language processing related to the real world, such as symbol grounding, language generation, and nonlinguistic data search by natural language queries. We argue that shogi (Japanese chess) commentaries, which are accompanied by game states, are an interesting testbed for these tasks. A commentator refers not only to the current board state but to past and future moves, and yet such references can be grounded in the game tree, possibly with the help of modern game-tree search algorithms. For this reason, we previously collected shogi commentaries together with board states and have been developing a game commentary generator. In this paper, we augment the corpus with manual annotation of modality expressions and event factuality. The annotated corpus includes 1,622 modality expressions, 5,014 event class tags and 3,092 factuality tags. It can be used to train a computer to identify words and phrases that signal factuality and to determine events with the said factuality, paving the way for grounding possible and counterfactual states.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Aspect-based Sentiment Evaluation of Chess Moves (ASSESS): an NLP-based Method for Evaluating Chess Strategies from Textbooks</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/aspect-based-sentiment-evaluation-of-chess-moves-assess-an-nlp-based-method-for-evaluating-chess-strategies-from-textbooks/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/aspect-based-sentiment-evaluation-of-chess-moves-assess-an-nlp-based-method-for-evaluating-chess-strategies-from-textbooks/</guid>
      <description>&lt;p&gt;The chess domain is well-suited for creating an artificial intelligence (AI) system that mimics real-world challenges, including decision-making. Throughout the years, minimal attention has been paid to investigating insights derived from unstructured chess data sources. In this study, we examine the complicated relationships between multiple referenced moves in a chess-teaching textbook, and propose a novel method designed to encapsulate chess knowledge derived from move-action phrases. This study investigates the feasibility of using a modified sentiment analysis method as a means for evaluating chess moves based on text. Our proposed Aspect-Based Sentiment Analysis (ABSA) method represents an advancement in evaluating the sentiment associated with referenced chess moves. By extracting insights from move-action phrases, our approach aims to provide a more fine-grained and contextually aware {\textquoteleft}chess move&amp;rsquo;-based sentiment classification. Through empirical experiments and analysis, we evaluate the performance of our fine-tuned ABSA model, presenting results that confirm the efficiency of our approach in advancing aspect-based sentiment classification within the chess domain. This research contributes to the area of game-playing by machines and shows the practical applicability of leveraging NLP techniques to understand the context of strategic games. Keywords: Natural Language Processing, Chess, Aspect-based Sentiment Analysis (ABSA), Chess Move Evaluation.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Audio Commentary System for Real-Time Racing Game Play</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/audio-commentary-system-for-real-time-racing-game-play/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/audio-commentary-system-for-real-time-racing-game-play/</guid>
      <description>&lt;p&gt;Live commentaries are essential for enhancing spectators&amp;rsquo; enjoyment and understanding during sports events or e-sports streams. We introduce a live audio commentator system designed specifically for a racing game, driven by the high demand in the e-sports field. While a player is playing a racing game, our system tracks real-time user play data including speed and steer rotations, and generates commentary to accompany the live stream. Human evaluation suggested that generated commentary enhances enjoyment and understanding of races compared to streams without commentary. Incorporating additional modules to improve diversity and detect irregular events, such as course-outs and collisions, further increases the preference for the output commentaries.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Automated cricket commentary generation using deep learning</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/automated-cricket-commentary-generation-using-deep-learning/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/automated-cricket-commentary-generation-using-deep-learning/</guid>
      <description>&lt;p&gt;This work presents an automated and novel system for cricket commentary generation by the introduction of event driven approach and image captioning features. The presented system uses artificial intelligence and machine learning (ML) based methods to investigate real-time match data and generate real-time commentary by Image Captioning technique. For this purpose, deep neural network-based models and digital image processing techniques are used to detect the significant moments in the real-time match and generate commentary based on these real-time match events. The proposed method has been assessed using a dataset of 2 hours live cricket matches of India and England. After processing the match video, it has been observed that the developed model is successfully able to generate high-quality real-time commentary with a significant amount of accuracy. By commissioning leading-edge deep neural network-based model, the developed model determines the fitness to generate subtitles that are not only precise but also contextually appropriate, and efficiently apprehending the essence of the input frames.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Automated Story Selection for Color Commentary in Sports</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/automated-story-selection-for-color-commentary-in-sports/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/automated-story-selection-for-color-commentary-in-sports/</guid>
      <description>&lt;p&gt;Automated sports commentary is a form of automated narrative. Sports commentary exists to keep the viewer informed and entertained. One way to entertain the viewer is by telling brief stories relevant to the game in progress. We present a system called the sports commentary recommendation system (SCoReS) that can automatically suggest stories for commentators to tell during games. Through several user studies, we compared commentary using SCoReS to three other types of commentary and show that SCoReS adds significantly to the broadcast across several enjoyment metrics. We also collected interview data from professional sports commentators who positively evaluated a demonstration of the system. We conclude that SCoReS can be a useful broadcast tool, effective at selecting stories that add to the enjoyment and watchability of sports. SCoReS is a step toward automating sports commentary and, thus, automating narrative.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Ball-by-Ball Cricket Commentary Generation using Stateful Sequence-to-Sequence Model</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/ball-by-ball-cricket-commentary-generation-using-stateful-sequence-to-sequence-model/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/ball-by-ball-cricket-commentary-generation-using-stateful-sequence-to-sequence-model/</guid>
      <description>&lt;p&gt;Due to the availability of high performance computational devices and enormous video data, deep learning algorithms are assisting for human understandable description of videos. Automatic commentary generation of cricket videos take advantage of aforementioned intelligent techniques. VGG-16 network facilitates extraction of visual pattern from frames followed by encoder-decoder LSTM model. Proposed model can handle variable length input data to output variable number of sequential output. Moreover, the model has ability to encompass temporal information to predict the line and length bowled by bowler, the shot selection of batsman and outcome of the ball. Due to unavailability of cricket commentary dataset, a novel cricket commentary dataset containing video-commentary pairs is presented. Evaluation is also performed on benchmark video captioning datasets which are Microsoft Video Description Dataset (MSVD) and MSR - Video to Text dataset (MSRVTT). Captions generated by our model are evaluated on video captioning metrics which are METEOR, BLEU, ROGUE L and CIDEr and outperforms the baseline model.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Commentary Generation from Data Records of Multiplayer Strategy Esports Game</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/commentary-generation-from-data-records-of-multiplayer-strategy-esports-game/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/commentary-generation-from-data-records-of-multiplayer-strategy-esports-game/</guid>
      <description>&lt;p&gt;Esports, a sports competition on video games, has become one of the most important sporting events. Although esports play logs have been accumulated, only a small portion of them accompany text commentaries for the audience to retrieve and understand the plays. In this study, we therefore introduce the task of generating game commentaries from esports&amp;rsquo; data records. We first build large-scale esports data-to-text datasets that pair structured data and commentaries from a popular esports game, League of Legends. We then evaluate Transformer-based models to generate game commentaries from structured data records, while examining the impact of the pre-trained language models. Evaluation results on our dataset revealed the challenges of this novel task. We will release our dataset to boost potential research in the data-to-text generation community.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Conceptual Representation and Evaluation of an FPS Game Commentary Generator</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/conceptual-representation-and-evaluation-of-an-fps-game-commentary-generator/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/conceptual-representation-and-evaluation-of-an-fps-game-commentary-generator/</guid>
      <description>&lt;p&gt;Playing video games has been popular across all the age limits of modern society. In the beginning, it was limited among the younger community and it was just a hobby limited to individuals. Even though the majority of society sees video gaming as having a negative impact on society, this modern industry acts a significant role in healing the present competitive, stressful society. Game commentary has played a major role in the domain of competitive ESports. A proper game commentary is beneficial to both players and the audience. The aim of this project is to analyze the gameplays to produce a commentary track while balancing the contributing factors, color commentary, and play-by-play commentary. The project consists of three modules that perform the study in three perspectives: 1. Word sets related to action, spatial, temporal, and statistical information, 2. Word sets related to color commentary, 3. Word sets related to play-by-play commentary. In each module, a game commentary is generated using only the word sets related to that module. For evaluation, the similarity between the human commentary and the generated commentaries individually will be calculated.&lt;/p&gt;</description>
    </item>
    <item>
      <title>CS-lol: a Dataset of Viewer Comment with Scene in E-sports Live-streaming</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/cs-lol-a-dataset-of-viewer-comment-with-scene-in-e-sports-live-streaming/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/cs-lol-a-dataset-of-viewer-comment-with-scene-in-e-sports-live-streaming/</guid>
      <description>&lt;p&gt;Billions of live-streaming viewers share their opinions on scenes they are watching in real-time and interact with the event, commentators as well as other viewers via text comments. Thus, there is necessary to explore viewers’ comments with scenes in E-sport live-streaming events. In this paper, we developed CS-lol, a new large-scale dataset containing comments from viewers paired with descriptions of game scenes in E-sports live-streaming. Moreover, we propose a task, namely viewer comment retrieval, to retrieve the viewer comments for the scene of the live-streaming event. Results on a series of baseline retrieval methods derived from typical IR evaluation methods show our task as a challenging task. Finally, we release CS-lol and baseline implementation to the research community as a resource.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Detection and labeling of bad moves for coaching go</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/detection-and-labeling-of-bad-moves-for-coaching-go/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/detection-and-labeling-of-bad-moves-for-coaching-go/</guid>
      <description>&lt;p&gt;The level of computer programs has now reached professional strength for many games, even for the game of Go recently. A more difficult task for computer intelligence now is to create a program able to coach human players, so that they can improve their play. In this paper, we propose a method to detect and label the bad moves of human players for the game of Go. This task is challenging because even strong human players only agree at a rate of around 50% about which moves should be considered as bad. We use supervised learning with features largely available in many Go programs, and we obtain an identification level close to the one observed between strong human players. Also, an evaluation by a professional player shows that our method is already useful for intermediate-level players.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Enhancing Commentary Strategies for Imperfect Information Card Games: A Study of Large Language Models in Guandan Commentary</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/enhancing-commentary-strategies-for-imperfect-information-card-games-a-study-of-large-language-models-in-guandan-commentary/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/enhancing-commentary-strategies-for-imperfect-information-card-games-a-study-of-large-language-models-in-guandan-commentary/</guid>
      <description>&lt;p&gt;Recent advancements in large language models (LLMs) have unlocked the potential for generating high-quality game commentary. However, producing insightful and engaging commentary for complex games with incomplete information remains a significant challenge. In this paper, we introduce a novel commentary method that combine Reinforcement Learning (RL) and LLMs, tailored specifically for the Chinese card game \textit{Guandan}. Our system leverages RL to generate intricate card-playing scenarios and employs LLMs to generate corresponding commentary text, effectively emulating the strategic analysis and narrative prowess of professional commentators. The framework comprises a state commentary guide, a Theory of Mind (ToM)-based strategy analyzer, and a style retrieval module, which seamlessly collaborate to deliver detailed and context-relevant game commentary in the Chinese language environment. We empower LLMs with ToM capabilities and refine both retrieval and information filtering mechanisms. This facilitates the generation of personalized commentary content. Our experimental results showcase the substantial enhancement in performance achieved by the proposed commentary framework when applied to open-source LLMs, surpassing the performance of GPT-4 across multiple evaluation metrics.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Extraction of Strong and Weak Regions of Cricket Batsman through Text-Commentary Analysis</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/extraction-of-strong-and-weak-regions-of-cricket-batsman-through-text-commentary-analysis/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/extraction-of-strong-and-weak-regions-of-cricket-batsman-through-text-commentary-analysis/</guid>
      <description>&lt;p&gt;Cricket is a famous game in the world where many metrics are introduced and being used to help the coaches and umpires to solve the critical problems. Though different statistics are used to quantify the player&amp;rsquo;s performance based on strike rate, average or for ranking, prediction, and optimal team selection. There is not any effective method to measure the strong and weak regions of a batsman in cricket. In this paper, a text mining method is presented to extract either the strong shot selection points that are frequent for scoring or weak shot regions that seem tough for a batsman to play. Also, a mechanism is put forward to calculate the region-wise strike rate of an individual batsman. To achieve the objectives, the T20 cricket text commentary is being used for this purpose which is available on the espncricinfo website. The proposed method can be helpful for coaches and players to know the strong or weak regions where the batsman feels ease or difficulty to play, respectively. Moreover, the opponent bowlers can also use this method to plan the area where to bowl to each batsman.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Fine-Grained Video Captioning via Graph-based Multi-Granularity Interaction Learning</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/fine-grained-video-captioning-via-graph-based-multi-granularity-interaction-learning/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/fine-grained-video-captioning-via-graph-based-multi-granularity-interaction-learning/</guid>
      <description>&lt;p&gt;Learning to generate continuous linguistic descriptions for multi-subject interactive videos in great details has particular applications in team sports auto-narrative. In contrast to traditional video caption, this task is more challenging as it requires simultaneous modeling of fine-grained individual actions, uncovering of spatio-temporal dependency structures of frequent group interactions, and then accurate mapping of these complex interaction details into long and detailed commentary. To explicitly address these challenges, we propose a novel framework &lt;italic&gt;Graph-based Learning for Multi-Granularity Interaction Representation (GLMGIR)&lt;/italic&gt; for fine-grained team sports auto-narrative task. A multi-granular interaction modeling module is proposed to extract among-subjects’ interactive actions in a progressive way for encoding both intra- and inter-team interactions. Based on the above multi-granular representations, a multi-granular attention module is developed to consider action/event descriptions of multiple spatio-temporal resolutions. Both modules are integrated seamlessly and work in a collaborative way to generate the final narrative. In the meantime, to facilitate reproducible research, we collect a new video dataset from &lt;italic&gt;YouTube.com&lt;/italic&gt; called Sports Video Narrative dataset (SVN). It is a novel direction as it contains &lt;inline-formula&gt;&lt;tex-math notation=&#34;LaTeX&#34;&gt;$6K$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;a href=&#34;mml:math&#34;&gt;mml:math&lt;/a&gt;&lt;a href=&#34;mml:mrow&#34;&gt;mml:mrow&lt;/a&gt;&lt;a href=&#34;mml:mn&#34;&gt;mml:mn&lt;/a&gt;6&amp;lt;/mml:mn&amp;gt;&lt;a href=&#34;mml:mi&#34;&gt;mml:mi&lt;/a&gt;K&amp;lt;/mml:mi&amp;gt;&amp;lt;/mml:mrow&amp;gt;&amp;lt;/mml:math&amp;gt;&lt;inline-graphic xlink:href=&#34;zhuang-ieq1-2946823.gif&#34;/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; team sports videos (i.e., NBA basketball games) with &lt;inline-formula&gt;&lt;tex-math notation=&#34;LaTeX&#34;&gt;$10K$&lt;/tex-math&gt;&lt;alternatives&gt;&lt;a href=&#34;mml:math&#34;&gt;mml:math&lt;/a&gt;&lt;a href=&#34;mml:mrow&#34;&gt;mml:mrow&lt;/a&gt;&lt;a href=&#34;mml:mn&#34;&gt;mml:mn&lt;/a&gt;10&amp;lt;/mml:mn&amp;gt;&lt;a href=&#34;mml:mi&#34;&gt;mml:mi&lt;/a&gt;K&amp;lt;/mml:mi&amp;gt;&amp;lt;/mml:mrow&amp;gt;&amp;lt;/mml:math&amp;gt;&lt;inline-graphic xlink:href=&#34;zhuang-ieq2-2946823.gif&#34;/&gt;&lt;/alternatives&gt;&lt;/inline-formula&gt; ground-truth narratives(e.g., sentences). Furthermore, as previous metrics such as METEOR (i.e., used in coarse-grained video caption task) DO NOT cope with fine-grained sports narrative task well, we hence develop a novel evaluation metric named Fine-grained Captioning Evaluation (FCE), which measures how accurate the generated linguistic description reflects fine-grained action details as well as the overall spatio-temporal interactional structure. Extensive experiments on our SVN dataset have demonstrated the effectiveness of the proposed framework for fine-grained team sports video auto-narrative.&lt;/p&gt;</description>
    </item>
    <item>
      <title>From eSports Data to Game Commentary: Datasets, Models, and Evaluation Metrics</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/from-esports-data-to-game-commentary-datasets-models-and-evaluation-metrics/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/from-esports-data-to-game-commentary-datasets-models-and-evaluation-metrics/</guid>
      <description>&lt;p&gt;Electronic sports (eSports), the sport competition using video games, has become one of the most popular sporting events now. The eSports audience needs textual commentaries for deeply understanding the games and for eﬃciently retrieving speciﬁc games of their interest. Therefore, in this work, we set up an eSports data-to-text generation task and tackle three fundamental problems: dataset construction, model design, and evaluation metrics. We ﬁrst build a data-to-text dataset containing data records and game commentaries from the a popular eSports game, League of Legends. On this new dataset, we propose a hierarchical model to address diﬃculty in handling long sequences of inputs and outputs with an encoder-decoder model. The hierarchical model sets multi-level encoders for the input data. Besides, we organize and design a new set of evaluation metrics including three aspects to meet this task’s goal. Experimental results on the new datasets conﬁrm that the hierarchical structure improves the performance of the model.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Game-MUG: Multimodal Oriented Game Situation Understanding and Commentary Generation Dataset</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/game-mug-multimodal-oriented-game-situation-understanding-and-commentary-generation-dataset/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/game-mug-multimodal-oriented-game-situation-understanding-and-commentary-generation-dataset/</guid>
      <description>&lt;p&gt;The dynamic nature of esports makes the situation relatively complicated for average viewers. Esports broadcasting involves game expert casters, but the caster-dependent game commentary is not enough to fully understand the game situation. It will be richer by including diverse multimodal esports information, including audiences&amp;rsquo; talks/emotions, game audio, and game match event information. This paper introduces GAME-MUG, a new multimodal game situation understanding and audience-engaged commentary generation dataset and its strong baseline. Our dataset is collected from 2020-2022 LOL game live streams from YouTube and Twitch, and includes multimodal esports game information, including text, audio, and time-series event logs, for detecting the game situation. In addition, we also propose a new audience conversation augmented commentary dataset by covering the game situation and audience conversation understanding, and introducing a robust joint multimodal dual learning model as a baseline. We examine the model&amp;rsquo;s game situation/event understanding ability and commentary generation capability to show the effectiveness of the multimodal aspects coverage and the joint integration learning approach.&lt;/p&gt;</description>
    </item>
    <item>
      <title>GameFlow: Narrative Visualization of NBA Basketball Games</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/gameflow-narrative-visualization-of-nba-basketball-games/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/gameflow-narrative-visualization-of-nba-basketball-games/</guid>
      <description>&lt;p&gt;ough basketball games have received broad attention, the forms of game reports and webcast are purely content-based cross-media: texts, videos, snapshots, and performance figures. Analytical narrations of games that seek to compose a complete game from heterogeneous datasets are challenging for general media producers because such a composition is time-consuming and heavily depends on domain experts. In particular, an appropriate analytical commentary of basketball games requires two factors, namely, rich context and domain knowledge, which includes game events, player locations, player profiles, and team pr&lt;/p&gt;</description>
    </item>
    <item>
      <title>Generating commentaries for tennis videos</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/generating-commentaries-for-tennis-videos/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/generating-commentaries-for-tennis-videos/</guid>
      <description>&lt;p&gt;esent an approach to automatically generating verbal commentaries for tennis games. We introduce a novel application that requires a combination of techniques from computer vision, natural language processing and machine learning. A video sequence is first analysed using state-of-the-art computer vision methods to track the ball, fit the detected edges to the court model, track the players, and recognise their strokes. Based on the recognised visual attributes we formulate the tennis commentary generation problem in the framework of long short-term memory recurrent neural networks as well as st&lt;/p&gt;</description>
    </item>
    <item>
      <title>Generating Live Soccer-Match Commentary from Play Data</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/generating-live-soccer-match-commentary-from-play-data/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/generating-live-soccer-match-commentary-from-play-data/</guid>
      <description>&lt;p&gt;We address the task of generating live soccer-match commentaries from play event data. This task has characteristics that (i) each commentary is only partially aligned with events, (ii) play event data contains many types of categorical and numerical attributes, (iii) live commentaries often mention player names and team names. For these reasons, we propose an encoder for play event data, which is enhanced with a gate mechanism. We also introduce an attention mechanism on events. In addition, we introduced placeholders and their reconstruction mechanism to enable the model to copy appropriate player names and team names from the input data. We conduct experiments on the play data of the English Premier League, provide a discussion on the result including generated commentaries.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Generating Racing Game Commentary from Vision, Language, and Structured Data</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/generating-racing-game-commentary-from-vision-language-and-structured-data/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/generating-racing-game-commentary-from-vision-language-and-structured-data/</guid>
      <description>&lt;p&gt;ask of automatically generating commentaries for races in a motor racing game, from vision, structured numerical, and textual data. Commentaries provide information to support spectators in understanding events in races. Commentary generation models need to interpret the race situation and generate the correct content at the right moment. We divide the task into two subtasks: utterance timing identification and utterance generation. Because existing datasets do not have such alignments of data in multiple modalities, this setting has not been explored in depth. In this study, we introduce a new large-scale dataset that contains aligned video data, structured numerical data, and transcribed commentaries that consist of 129,226 utterances in 1,389 races in a game. Our analysis reveals that the characteristics of commentaries change over time or from viewpoints. Our experiments on the subtasks show that it is still challenging for a state-of-the-art vision encoder to capture useful information from videos to generate accurate commentaries. We make the dataset and baseline implementation publicly available for further research.&lt;/p&gt;</description>
    </item>
    <item>
      <title>GOAL: A Challenging Knowledge-grounded Video Captioning Benchmark for Real-time Soccer Commentary Generation</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/goal-a-challenging-knowledge-grounded-video-captioning-benchmark-for-real-time-soccer-commentary-generation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/goal-a-challenging-knowledge-grounded-video-captioning-benchmark-for-real-time-soccer-commentary-generation/</guid>
      <description>&lt;p&gt;-&lt;/p&gt;
&lt;p&gt;Despite the recent emergence of video captioning models, how to generate vivid, fine-grained video descriptions based on the background knowledge (i.e., long and informative commentary about the domain-specific scenes with appropriate reasoning) is still far from being solved, which however has great applications such as automatic sports narrative. Based on soccer game videos and synchronized commentary data, we present GOAL, a benchmark of over 8.9k soccer video clips, 22k sentences, and 42k knowledge triples for proposing a challenging new task setting as Knowledge-grounded Video Captioning (KGVC). We experimentally test existing state-of-the-art (SOTA) methods&lt;/p&gt;</description>
    </item>
    <item>
      <title>Learning a game commentary generator with grounded move expressions</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/learning-a-game-commentary-generator-with-grounded-move-expressions/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/learning-a-game-commentary-generator-with-grounded-move-expressions/</guid>
      <description>&lt;p&gt;This paper describes a machine learning-based approach for generating natural language comments on Shogi games. We generate comments by using a discriminative language model trained with a large amount of Shogi game records and comments made by human experts. Central to our method is accurate mapping of move expressions appearing in experts&amp;rsquo; comments to game states (i.e. positions) of Shogi, because the discriminative language model is trained with textual expressions paired with corresponding Shogi positions. We describe how such mapping can be performed by using evaluation information obtained from a Shogi program. Experimental results show that we can actually generate helpful comments for some positions.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Learning to Generate Move-by-Move Commentary for Chess Games from Large-Scale Social Forum Data</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/learning-to-generate-move-by-move-commentary-for-chess-games-from-large-scale-social-forum-data/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/learning-to-generate-move-by-move-commentary-for-chess-games-from-large-scale-social-forum-data/</guid>
      <description>&lt;p&gt;r examines the problem of generating natural language descriptions of chess games. We introduce a new large-scale chess commentary dataset and propose methods to generate commentary for individual moves in a chess game. The introduced dataset consists of more than 298K chess move-commentary pairs across 11K chess games. We highlight how this task poses unique research challenges in natural language generation: the data contain a large variety of styles of commentary and frequently depend on pragmatic context. We benchmark various baselines and propose an end-to-end trainable neural model which takes into account multiple pragmatic aspects of&lt;/p&gt;</description>
    </item>
    <item>
      <title>Learning to sportscast: a test of grounded language acquisition</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/learning-to-sportscast-a-test-of-grounded-language-acquisition/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/learning-to-sportscast-a-test-of-grounded-language-acquisition/</guid>
      <description>&lt;p&gt;t a novel commentator system that learns language from sportscasts of simulated soccer games. The system learns to parse and generate commentaries without any engineered knowledge about the English language. Training is done using only ambiguous supervision in the form of textual human commentaries and simulation states of the soccer games. The system simultaneously tries to establish correspondences between the commentaries and the simulation states as well as build a translation model. We also present a novel algorithm, Iterative Generation Strategy Learning (IGSL), for deciding which events to comment on. Human evaluat&lt;/p&gt;</description>
    </item>
    <item>
      <title>LoL-V2T: Large-Scale Esports Video Description Dataset</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/lol-v2t-large-scale-esports-video-description-dataset/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/lol-v2t-large-scale-esports-video-description-dataset/</guid>
      <description>&lt;p&gt;Esports is a fastest-growing new field with a largely online-presence, and is creating a demand for automatic domain-specific captioning tools. However, at the current time, there are few approaches that tackle the esports video description problem. In this work, we propose a large-scale dataset for esports video description, focusing on the popular game &amp;ldquo;League of Legends&amp;rdquo;. The dataset, which we call LoL-V2T, is the largest video description dataset in the video game domain, and includes 9,723 clips with 62,677 captions. This new dataset presents multiple new video captioning challenges such as large amounts&lt;/p&gt;</description>
    </item>
    <item>
      <title>MatchTime: Towards Automatic Soccer Game Commentary Generation</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/matchtime-towards-automatic-soccer-game-commentary-generation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/matchtime-towards-automatic-soccer-game-commentary-generation/</guid>
      <description>&lt;p&gt;Soccer is a globally popular sport with a vast audience, in this paper, we consider constructing an automatic soccer game commentary model to improve the audiences&amp;rsquo; viewing experience. In general, we make the following contributions: &lt;em&gt;First&lt;/em&gt;, observing the prevalent video-text misalignment in existing datasets, we manually annotate timestamps for 49 matches, establishing a more robust benchmark for soccer game commentary generation, termed as &lt;em&gt;SN-Caption-test-align&lt;/em&gt;; &lt;em&gt;Second&lt;/em&gt;, we propose a multi-modal temporal alignment pipeline to automatically correct and filter the existing dataset at scale, creating a higher-quality soccer game commentary dataset for training, denoted as &lt;em&gt;MatchTime&lt;/em&gt;; &lt;em&gt;Third&lt;/em&gt;, based on our curated dataset, we train an automatic commentary generation model, named &lt;strong&gt;MatchVoice&lt;/strong&gt;. Extensive experiments and ablation studies have demonstrated the effectiveness of our alignment pipeline, and training model on the curated datasets achieves state-of-the-art performance for commentary generation, showcasing that better alignment can lead to significant performance improvements in downstream tasks.&lt;/p&gt;</description>
    </item>
    <item>
      <title>MOBA-E2C: Generating MOBA Game Commentaries via Capturing Highlight Events from the Meta-Data</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/moba-e2c-generating-moba-game-commentaries-via-capturing-highlight-events-from-the-meta-data/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/moba-e2c-generating-moba-game-commentaries-via-capturing-highlight-events-from-the-meta-data/</guid>
      <description>&lt;p&gt;MOBA (Multiplayer Online Battle Arena) games such as Dota2 are currently one of the most popular e-sports gaming genres. Following professional commentaries is a great way to understand and enjoy a MOBA game. However, massive game competitions lack commentaries because of the shortage of professional human commentators. As an alternative, employing machine commentators that can work at any time and place is a feasible solution. Considering the challenges in modeling MOBA games, we propose a data-driven MOBA commentary generation framework, MOBA-E2C, allowing a model to generate commentaries based on the game meta-data. Subsequently, to alleviate the burden of collecting supervised data, we propose a MOBA-FuseGPT generator to generate MOBA game commentaries by fusing the power of a rule-based generator and a generative GPT generator. Finally, in the experiments, we take a popular MOBA game Dota2 as our case and construct a Chinese Dota2 commentary generation dataset Dota2-Commentary. Experimental results demonstrate the superior performance of our approach. To the best of our knowledge, this work is the first Dota2 machine commentator and Dota2-Commentary is the first dataset.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Multimodal Joint Emotion and Game Context Recognition in League of Legends Livestreams</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/multimodal-joint-emotion-and-game-context-recognition-in-league-of-legends-livestreams/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/multimodal-joint-emotion-and-game-context-recognition-in-league-of-legends-livestreams/</guid>
      <description>&lt;p&gt;ming provides the viewer with a rich set of audio-visual data, conveying information both with regards to the game itself, through game footage and audio, as well as the streamer&amp;rsquo;s emotional state and behaviour via webcam footage and audio. Analysing player behaviour and discovering correlations with game context is crucial for modelling and understanding important aspects of livestreams, but comes with a significant set of challenges - such as fusing multimodal data captured by different sensors in uncontrolled (`in-the-wild&amp;rsquo;) conditions. Firstly, we present, to our knowledge, the first data set of League of Legends livestreams, annotat&lt;/p&gt;</description>
    </item>
    <item>
      <title>Outcome Classification in Cricket Using Deep Learning</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/outcome-classification-in-cricket-using-deep-learning/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/outcome-classification-in-cricket-using-deep-learning/</guid>
      <description>&lt;p&gt;n applications of Artificial Intelligence day by day, every domain is going automated. Machine learning has enabled systems to learn the process on its own in order to reduce the human labour. In sports like cricket, Football AI has not been used on a greater scale but there are certain areas where it can be of great help to apply AI techniques. In this paper the outcome classification task has been performed on cricket videos. The main purpose of performing such activities is to create automatic commentary generation. There are many sub-tasks needed to be considered for this task. One of those tasks is to cl&lt;/p&gt;</description>
    </item>
    <item>
      <title>Scaling up SoccerNet with Multi-view Spatial Localization and Re-identification</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/scaling-up-soccernet-with-multi-view-spatial-localization-and-re-identification/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/scaling-up-soccernet-with-multi-view-spatial-localization-and-re-identification/</guid>
      <description>&lt;p&gt;Soccer videos are a rich playground for computer vision, involving many elements, such as players, lines, and specific objects. Hence, to capture the richness of this sport and allow for fine automated analyses, we release SoccerNet-v3, a major extension of the SoccerNet dataset, providing a wide variety of spatial annotations and cross-view correspondences. SoccerNet’s broadcast videos contain replays of important actions, allowing us to retrieve a same action from different viewpoints. We annotate those live and replay action frames showing same moments with exhaustive local information. Specifically, we label lines, goal parts, players, referees, teams, salient objects, jersey numbers, and we establish player correspondences between the views. This yields 1,324,732 annotations on 33,986 soccer images, making SoccerNet-v3 the largest dataset for multi-view soccer analysis. Derived tasks may benefit from these annotations, like camera calibration, player localization, team discrimination and multi-view re-identification, which can further sustain practical applications in augmented reality and soccer analytics. Finally, we provide Python codes to easily download our data and access our annotations.&lt;/p&gt;</description>
    </item>
    <item>
      <title>SentiMATE: Learning to play Chess through Natural Language Processing</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/sentimate-learning-to-play-chess-through-natural-language-processing/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/sentimate-learning-to-play-chess-through-natural-language-processing/</guid>
      <description>&lt;p&gt;We present SentiMATE, a novel end-to-end Deep Learning model for Chess, employing Natural Language Processing that aims to learn an effective evaluation function assessing move quality. This function is pre-trained on the sentiment of commentary associated with the training moves and is used to guide and optimize the agent&amp;rsquo;s game-playing decision making. The contributions of this research are three-fold: we build and put forward both a classifier which extracts commentary describing the quality of Chess moves in vast commentary datasets, and a Sentiment Analysis model trained on Chess commentary to accurately predict the quality of said moves, to then use those predictions to evaluate the optimal next move of a Chess agent. Both classifiers achieve over 90 which evaluates Chess moves based on a pre-trained sentiment evaluation function. Our results exhibit strong evidence to support our initial hypothesis - &amp;ldquo;Can Natural Language Processing be used to train a novel and sample efficient evaluation function in Chess Engines?&amp;rdquo; - as we integrate our evaluation function into modern Chess engines and play against agents with traditional Chess move evaluation functions, beating both random agents and a DeepChess implementation at a level-one search depth - representing the number of moves a traditional Chess agent (employing the alpha-beta search algorithm) looks ahead in order to evaluate a given chess state.&lt;/p&gt;</description>
    </item>
    <item>
      <title>SOCCER: An Information-Sparse Discourse State Tracking Collection in the Sports Commentary Domain</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/soccer-an-information-sparse-discourse-state-tracking-collection-in-the-sports-commentary-domain/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/soccer-an-information-sparse-discourse-state-tracking-collection-in-the-sports-commentary-domain/</guid>
      <description>&lt;p&gt;In the pursuit of natural language understanding, there has been a long standing interest in tracking state changes throughout narratives. Impressive progress has been made in modeling the state of transaction-centric dialogues and procedural texts. However, this problem has been less intensively studied in the realm of general discourse where ground truth descriptions of states may be loosely defined and state changes are less densely distributed over utterances. This paper proposes to turn to simplified, fully observable systems that show some of these properties: Sports events. We curated 2,263 soccer matches including time-stamped natural language commentary accompanied by discrete events such as a team scoring goals, switching players or being penalized with cards. We propose a new task formulation where, given paragraphs of commentary of a game at different timestamps, the system is asked to recognize the occurrence of in-game events. This domain allows for rich descriptions of state while avoiding the complexities of many other real-world settings. As an initial point of performance measurement, we include two baseline methods from the perspectives of sentence classification with temporal dependence and current state-of-the-art generative model, respectively, and demonstrate that even sophisticated existing methods struggle on the state tracking task when the definition of state broadens or non-event chatter becomes prevalent.&lt;/p&gt;</description>
    </item>
    <item>
      <title>SoccerNet-Caption: Dense Video Captioning for Soccer Broadcasts Commentaries</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/soccernet-caption-dense-video-captioning-for-soccer-broadcasts-commentaries/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/soccernet-caption-dense-video-captioning-for-soccer-broadcasts-commentaries/</guid>
      <description>&lt;p&gt;Soccer is more than just a game - it is a passion that transcends borders and unites people worldwide. From the roar of the crowds to the excitement of the commentators, every moment of a soccer match is a thrill. Yet, with so many games happening simultaneously, fans cannot watch them all live. Notifications for main actions can help, but lack the engagement of live commentary, leaving fans feeling disconnected. To fulfill this need, we propose in this paper a novel task of dense video captioning focusing on the generation of textual commentaries anchored with single times-tamps. To support this task, we additionally present a challenging dataset consisting of almost 37k timestamped commentaries across 715.9 hours of soccer broadcast videos. Additionally, we propose a first benchmark and baseline for this task, highlighting the difficulty of temporally anchoring commentaries yet showing the capacity to generate meaningful commentaries. By providing broadcasters with a tool to summarize the content of their video with the same level of engagement as a live game, our method could help satisfy the needs of the numerous fans who follow their team but cannot necessarily watch the live game. We believe our method has the potential to enhance the accessibility and understanding of soccer content for a wider audience, bringing the excitement of the game to more people.&lt;/p&gt;</description>
    </item>
    <item>
      <title>SoccerNet-Echoes: A Soccer Game Audio Commentary Dataset</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/soccernet-echoes-a-soccer-game-audio-commentary-dataset/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/soccernet-echoes-a-soccer-game-audio-commentary-dataset/</guid>
      <description>&lt;p&gt;The application of Automatic Speech Recognition (ASR) technology in soccer enables sports analytics by extracting audio commentaries to provide insights into game events and facilitate automatic game understanding. This paper presents SoccerNet-Echoes, an extension of the SoccerNet dataset with automatically generated transcriptions of soccer game broadcasts. Generated using the Whisper model and translated with Google Translate into English when needed, these transcriptions enhance video content with textual information derived from game audio. SoccerNet-Echoes serves as a comprehensive resource for developing algorithms in action spotting, caption generation, and game summarization. Through a series of experiments, we demonstrate that combining modalities—audio, video, and text—yields mixed results on classification tasks. The combination of audio and video shows improved performance over individual modalities, while the addition of ASR text does not significantly enhance results. Additionally, our baseline summarization tasks indicate that ASR content enriches summaries, offering insights beyond event information. This multimodal dataset supports diverse applications, broadening the scope of research in sports analytics. The dataset is available at: &lt;a href=&#34;https://github.com/SoccerNet/sn-echoes&#34;&gt;https://github.com/SoccerNet/sn-echoes&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>SoccerNet-v2: A Dataset and Benchmarks for Holistic Understanding of Broadcast Soccer Videos</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/soccernet-v2-a-dataset-and-benchmarks-for-holistic-understanding-of-broadcast-soccer-videos/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/soccernet-v2-a-dataset-and-benchmarks-for-holistic-understanding-of-broadcast-soccer-videos/</guid>
      <description>&lt;p&gt;Understanding broadcast videos is a challenging task in computer vision, as it requires generic reasoning capabilities to appreciate the content offered by the video editing. In this work, we propose SoccerNet-v2, a novel large-scale corpus of manual annotations for the SoccerNet [24] video dataset, along with open challenges to encourage more research in soccer understanding and broadcast production. Specifically, we release around 300k annotations within SoccerNet’s 500 untrimmed broadcast soccer videos. We extend current tasks in the realm of soccer to include action spotting, camera shot segmentation with boundary detection, and we define a novel replay grounding task. For each task, we provide and discuss benchmark results, reproducible with our open-source adapted implementations of the most relevant works in the field. SoccerNet-v2 is presented to the broader research community to help push computer vision closer to automatic solutions for more general video understanding and production purposes.&lt;/p&gt;</description>
    </item>
    <item>
      <title>SoccerNet: A Scalable Dataset for Action Spotting in Soccer Videos</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/soccernet-a-scalable-dataset-for-action-spotting-in-soccer-videos/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/soccernet-a-scalable-dataset-for-action-spotting-in-soccer-videos/</guid>
      <description>&lt;p&gt;In this paper, we introduce SoccerNet, a benchmark for action spotting in soccer videos. The dataset is composed of 500 complete soccer games from six main European leagues, covering three seasons from 2014 to 2017 and a total duration of 764 hours. A total of 6,637 temporal annotations are automatically parsed from online match reports at a one minute resolution for three main classes of events (Goal, Yellow/Red Card, and Substitution). As such, the dataset is easily scalable. These annotations are manually refined to a one second resolution by anchoring them at a single timestamp following well-defined soccer rules. With an average of one event every 6.9 minutes, this dataset focuses on the problem of localizing very sparse events within long videos. We define the task of spotting as finding the anchors of soccer events in a video. Making use of recent developments in the realm of generic action recognition and detection in video, we provide strong baselines for detecting soccer events. We show that our best model for classifying temporal segments of length one minute reaches a mean Average Precision (mAP) of 67.8%. For the spotting task, our baseline reaches an Average-mAP of 49.7% for tolerances d ranging from 5 to 60 seconds. Our dataset and models are available at &lt;a href=&#34;https://silviogiancola.github.io/SoccerNet&#34;&gt;https://silviogiancola.github.io/SoccerNet&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Sports Commentary Recommendation System (SCoReS): machine learning for automated narrative</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/sports-commentary-recommendation-system-scores-machine-learning-for-automated-narrative/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/sports-commentary-recommendation-system-scores-machine-learning-for-automated-narrative/</guid>
      <description>&lt;p&gt;Automated sports commentary is a form of automated narrative. Sports commentary exists to keep the viewer informed and entertained. One way to entertain the viewer is by telling brief stories relevant to the game in progress. We introduce a system called the Sports Commentary Recommendation System (SCoReS) that can automatically suggest stories for commentators to tell during games. Through several user studies, we compared commentary using SCoReS to three other types of commentary and show that SCoReS adds significantly to the broadcast across several enjoyment metrics. We also collected interview data from professional sports commentators who positively evaluated a demonstration of the system. We conclude that SCoReS can be a useful broadcast tool, effective at selecting stories that add to the enjoyment and watch-ability of sports. SCoReS is a step toward automating sports commentary and, thus, automating narrative.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Story Selection and Recommendation System for Colour Commentary in Cricket</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/story-selection-and-recommendation-system-for-colour-commentary-in-cricket/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/story-selection-and-recommendation-system-for-colour-commentary-in-cricket/</guid>
      <description>&lt;p&gt;During a cricket match, commentary keeps the viewers entertained and updated about the game. Quoting relevant stories related to the current game scenario makes the game more interesting. But the knowledge of commentator, however vast for a human being, is still limited relative to the total set of available stories. A major challenge is to narrate the most relevant stories (past incidents) based on the current (never-before-seen) game state. The paper proposes a solution which is an AI based approach that will assist the colour commentators in effective storytelling that is interesting to the audience, and related to what is actually happening in the game being broadcast.&lt;/p&gt;</description>
    </item>
    <item>
      <title>TenniSet: A Dataset for Dense Fine-Grained Event Recognition, Localisation and Description</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/tenniset-a-dataset-for-dense-fine-grained-event-recognition-localisation-and-description/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/tenniset-a-dataset-for-dense-fine-grained-event-recognition-localisation-and-description/</guid>
      <description>&lt;p&gt;This paper introduces a new video understanding dataset which can be utilised for the related problems of event recognition, localisation and description in video. Our dataset consists of dense, well structured event annotations in untrimmed video of tennis matches. We also include highly detailed commentary style descriptions, which are heavily dependent on both the occurrence as well as the sequence of particular events. We use general deep learning techniques to acquire some initial baseline results on our dataset, without the need for explicit domain-specific assumptions.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Training a Multilingual Sportscaster: Using Perceptual Context to Learn Language</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/training-a-multilingual-sportscaster-using-perceptual-context-to-learn-language/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/training-a-multilingual-sportscaster-using-perceptual-context-to-learn-language/</guid>
      <description>&lt;p&gt;We present a novel framework for learning to interpret and generate language using only perceptual context as supervision. We demonstrate its capabilities by developing a system that learns to sportscast simulated robot soccer games in both English and Korean without any language-specific prior knowledge. Training employs only ambiguous supervision consisting of a stream of descriptive textual comments and a sequence of events extracted from the simulation trace. The system simultaneously establishes correspondences between individual comments and the events that they describe while building a translation model that supports both parsing and generation. We also present a novel algorithm for learning which events are worth describing. Human evaluations of the generated commentaries indicate they are of reasonable quality and in some cases even on par with those produced by humans for our limited domain.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Understanding Game-Playing Agents with Natural Language Annotations</title>
      <link>http://localhost:1313/AIGGC-datasheet/datasets/understanding-game-playing-agents-with-natural-language-annotations/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/AIGGC-datasheet/datasets/understanding-game-playing-agents-with-natural-language-annotations/</guid>
      <description>&lt;p&gt;We present a new dataset containing 10K human-annotated games of Go and show how these natural language annotations can be used as a tool for model interpretability. Given a board state and its associated comment, our approach uses linear probing to predict mentions of domain-specific terms (e.g., ko, atari) from the intermediate state representations of game-playing agents like AlphaGo Zero. We find these game concepts are nontrivially encoded in two distinct policy networks, one trained via imitation learning and another trained via reinforcement learning. Furthermore, mentions of domain-specific terms are most easily predicted from the later layers of both models, suggesting that these policy networks encode high-level abstractions similar to those used in the natural language annotations.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
